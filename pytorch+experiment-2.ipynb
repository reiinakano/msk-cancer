{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import os \n",
    "import time\n",
    "import joblib\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Define adjustable parameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "EMBEDDING_DIM = 64\n",
    "LSTM_OUT_DIM = 64\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT_PROB = 0.5\n",
    "LINEAR_HIDDEN_SIZE = 64\n",
    "\n",
    "TOKENIZER_TOP_N_WORDS = 2000  # Increasing this would increase embedding size\n",
    "TEXT_WORD_LIMIT = 2000\n",
    "\n",
    "CUDA_AVAILABLE = torch.cuda.is_available()\n",
    "\n",
    "PRINT_FREQUENCY = 1\n",
    "\n",
    "CHECKPOINT_FOLDER = 'pytorchckpts/4newarchsmall'\n",
    "\n",
    "NUM_EPOCHS = 5  # You can change this to number of epochs you want the model to go through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do not change unless you know what this does. No need to change this even when resuming\n",
    "BEST_LOSS = np.inf \n",
    "EPOCH = 0 \n",
    "CHECKPOINT_NAME = os.path.join(CHECKPOINT_FOLDER, 'checkpoint.pth.tar')\n",
    "BEST_CHECKPOINT_NAME = os.path.join(CHECKPOINT_FOLDER, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load and merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_train_txt = pd.read_csv('training_text', sep='\\|\\|', header=None, skiprows=1, names=[\"ID\",\"Text\"], engine='python')\n",
    "df_train_var = pd.read_csv('training_variants')\n",
    "df_test_txt = pd.read_csv('test_text', sep='\\|\\|', header=None, skiprows=1, names=[\"ID\",\"Text\"], engine='python')\n",
    "df_test_var = pd.read_csv('test_variants')\n",
    "df_train = pd.merge(df_train_var, df_train_txt, how='left', on='ID')\n",
    "df_test = pd.merge(df_test_var, df_test_txt, how='left', on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Split off a validation set\n",
    "df_train, df_val = train_test_split(df_train, test_size = 0.2, random_state = 42, stratify=df_train['Class'].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class SentencesDataset(Dataset):\n",
    "    def __init__(self, df, word_to_ix, word_limit=2000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: pandas dataframe of same format as df_train/df_test\n",
    "            \n",
    "            word_to_ix: word to index dictionary\n",
    "            \n",
    "            word_limit: Number of words to limit\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.word_to_ix = word_to_ix\n",
    "        self.word_limit = word_limit\n",
    "        if 'Class' in df:\n",
    "            self.le = LabelEncoder().fit(df['Class'].values)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df['Text'].values[idx]\n",
    "        gene = self.df['Gene'].values[idx]\n",
    "        variation = self.df['Variation'].values[idx]\n",
    "        \n",
    "        # Tokenize text. If word count becomes greater than limit, break.\n",
    "        num_words = 0\n",
    "        encoded_tokenized_text = []\n",
    "        gene_same = []\n",
    "        variation_same = []\n",
    "        for sent in nltk.sent_tokenize(text):\n",
    "            for word in nltk.word_tokenize(sent):\n",
    "                if word in self.word_to_ix:\n",
    "                    encoded_tokenized_text.append(self.word_to_ix[word])\n",
    "                    num_words += 1\n",
    "                    if word == gene:\n",
    "                        gene_same.append(1)\n",
    "                    else:\n",
    "                        gene_same.append(0)\n",
    "                    if word == variation:\n",
    "                        variation_same.append(1)\n",
    "                    else:\n",
    "                        variation_same.append(0)\n",
    "                    \n",
    "                else:  # Word not in word_to_ix. Still, you must check if word is the gene/variation or not\n",
    "                    if word == gene or word == variation:\n",
    "                        if word == gene:\n",
    "                            gene_same.append(1)\n",
    "                        else:\n",
    "                            gene_same.append(0)\n",
    "                        if word == variation:\n",
    "                            variation_same.append(1)\n",
    "                        else:\n",
    "                            variation_same.append(0)\n",
    "                        encoded_tokenized_text.append(self.word_to_ix['Unknown'])\n",
    "                        num_words += 1\n",
    "                        \n",
    "                if num_words >= self.word_limit: break\n",
    "                \n",
    "            if num_words >= self.word_limit: break\n",
    "        \n",
    "        # Special case: number of tokenized words = 0. Change to single word of unknown\n",
    "        if num_words == 0:\n",
    "            encoded_tokenized_text = [self.word_to_ix['Unknown']]\n",
    "            gene_same.append(0)\n",
    "            variation_same.append(0)\n",
    "            print('FOUND NULL SENTENCE!!!')\n",
    "            num_words += 1\n",
    "        \n",
    "        # Pad tokenized text if needed\n",
    "        if num_words < self.word_limit:\n",
    "            encoded_tokenized_text += [0] * (self.word_limit - num_words)\n",
    "            gene_same += [0] * (self.word_limit - num_words)\n",
    "            variation_same += [0] * (self.word_limit - num_words)\n",
    "        \n",
    "        # Create sample\n",
    "        sample = {'text': encoded_tokenized_text,\n",
    "                  'gene_same': gene_same,\n",
    "                  'variation_same': variation_same,\n",
    "                  'length': num_words,\n",
    "                  'gene': self.word_to_ix[gene] if gene in self.word_to_ix else self.word_to_ix['Unknown'],\n",
    "                  'variation': self.word_to_ix[variation] if variation in self.word_to_ix else self.word_to_ix['Unknown']}\n",
    "        \n",
    "        # If contains class, include class in sample\n",
    "        if 'Class' in self.df:\n",
    "            sample['class'] =  self.le.transform([self.df['Class'].values[idx]])[0]\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build word to index dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_word_to_ix(df_train, location, top_n_words=TOKENIZER_TOP_N_WORDS):\n",
    "    \"\"\"Builds word_to_ix dictionary and saves it in location\"\"\"\n",
    "    if os.path.exists(location):\n",
    "        print('found pickled word_to_ix')\n",
    "        return joblib.load(location)\n",
    "    \n",
    "    word_counts = dict()\n",
    "\n",
    "    for doc in df_train['Text'].values:\n",
    "        for sent in nltk.sent_tokenize(doc):\n",
    "            for word in nltk.word_tokenize(sent):\n",
    "                if word not in word_counts:\n",
    "                    word_counts[word] = 1\n",
    "                else:\n",
    "                    word_counts[word] += 1\n",
    "    \n",
    "    wcounts = list(word_counts.items())\n",
    "    wcounts.sort(key=lambda x: x[1], reverse=True)\n",
    "    sorted_voc = [wc[0] for wc in wcounts]\n",
    "    word_to_ix = dict(list(zip(sorted_voc, list(range(top_n_words)))))\n",
    "    \n",
    "    ix = len(word_to_ix)\n",
    "\n",
    "    for gene in df_train['Gene'].values:\n",
    "        if gene not in word_to_ix:\n",
    "            word_to_ix[gene] = ix\n",
    "            ix += 1\n",
    "\n",
    "    for variation in df_train['Variation'].values:\n",
    "        if variation not in word_to_ix:\n",
    "            word_to_ix[variation] = ix\n",
    "            ix += 1\n",
    "    \n",
    "    if 'Unknown' not in word_to_ix:\n",
    "        word_to_ix['Unknown'] = ix\n",
    "    \n",
    "    joblib.dump(word_to_ix, location, compress=3)\n",
    "    \n",
    "    return word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found pickled word_to_ix\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = build_word_to_ix(df_train, 'word_to_ix_3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_dataset = SentencesDataset(df_train, word_to_ix, word_limit=TEXT_WORD_LIMIT)\n",
    "train_dataset_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_dataset = SentencesDataset(df_val, word_to_ix, word_limit=TEXT_WORD_LIMIT)\n",
    "val_dataset_loader = DataLoader(val_dataset, batch_size=45, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([3000, 32])\n",
      "(array([0, 1]), array([94808,  1192]))\n",
      "(array([0, 1]), array([95952,    48]))\n",
      "3000\n",
      "1\n",
      "torch.Size([3000, 32])\n",
      "(array([0, 1]), array([94515,  1485]))\n",
      "(array([0, 1]), array([95901,    99]))\n",
      "3000\n",
      "2\n",
      "torch.Size([3000, 32])\n",
      "(array([0, 1]), array([94840,  1160]))\n",
      "(array([0, 1]), array([95859,   141]))\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_dataset_loader):\n",
    "    print(i)\n",
    "    print(torch.stack(batch['gene_same']).size())\n",
    "    print(np.unique(torch.stack(batch['gene_same']).numpy(), return_counts=True))\n",
    "    print(np.unique(torch.stack(batch['variation_same']).numpy(), return_counts=True))\n",
    "    print(len(batch['text']))\n",
    "    if i > 1: \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_out_dim, bidirectional, prob):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm_out_dim = lstm_out_dim\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim+2, lstm_out_dim, bidirectional=bidirectional)\n",
    "        self.linear = nn.Linear(lstm_out_dim*self.num_directions+2*embedding_dim, 9)\n",
    "        self.dropout = nn.Dropout(p=prob)  \n",
    "        \n",
    "    def forward(self, batch):\n",
    "        batch_size = len(batch['length'])\n",
    "        \n",
    "        # Sort all tensors by length of the sequence in decreasing order (for packed padded sequences to work)\n",
    "        length, indices = torch.sort(batch['length'], dim=0, descending=True)\n",
    "        gene = batch['gene'][indices]\n",
    "        variation = batch['variation'][indices]\n",
    "        text_batch = torch.stack(batch['text'], 0)[:, indices]\n",
    "        gene_same = torch.stack(batch['gene_same'], 0)[:, indices]\n",
    "        variation_same = torch.stack(batch['variation_same'], 0)[:, indices]\n",
    "        \n",
    "        # Wrap all tensors around a variable. Send to GPU if possible.\n",
    "        text_batch = Variable(text_batch)\n",
    "        length = Variable(length)\n",
    "        gene = Variable(gene)\n",
    "        variation = Variable(variation)\n",
    "        gene_same = Variable(gene_same)\n",
    "        variation_same = Variable(variation_same)\n",
    "        if CUDA_AVAILABLE:\n",
    "            text_batch, length, gene, variation, gene_same, variation_same = \\\n",
    "                text_batch.cuda(), length.cuda(), gene.cuda(), variation.cuda(), gene_same.cuda(), variation_same.cuda()\n",
    "        \n",
    "        # Pass text, gene, and variation to embedding\n",
    "        embedded_text = self.embedding(text_batch)\n",
    "        embedded_gene = self.embedding(gene)\n",
    "        embedded_variation = self.embedding(variation)\n",
    "        \n",
    "        # Concatenate gene + variation with embedded text\n",
    "        concatenated_embedded = torch.cat([embedded_text,\n",
    "                                           torch.unsqueeze(gene_same, dim=2).float(),\n",
    "                                           torch.unsqueeze(variation_same, dim=2).float()\n",
    "                                          ],\n",
    "                                          dim=2)\n",
    "        \n",
    "        # Initialize hidden and cell states for LSTM\n",
    "        h0, c0 = self.init_hidden_and_cell_states(batch_size, 1, self.num_directions, self.lstm_out_dim)\n",
    "        \n",
    "        # Pack sequence, run through LSTM, and unpack output\n",
    "        packed_embedded_text = pack_padded_sequence(concatenated_embedded, list(length.data))\n",
    "        packed_h, (packed_h_t, packed_c_t) = self.lstm(packed_embedded_text, (h0, c0))\n",
    "        h, _ = pad_packed_sequence(packed_h, batch_first=True)\n",
    "        \n",
    "        # Use fancy indexing to retrieve LSTM outputs for last timestep in each sequence\n",
    "        h = h[\n",
    "            np.arange(batch_size).reshape(-1, 1).tolist(), \n",
    "            length.data.view(-1, 1) - 1, \n",
    "            list(range(self.lstm_out_dim*self.num_directions))\n",
    "        ]\n",
    "        \n",
    "        concatenated_h = torch.cat(\n",
    "            [\n",
    "                h,\n",
    "                embedded_gene,\n",
    "                embedded_variation\n",
    "            ],\n",
    "            dim=1\n",
    "        )\n",
    "        \n",
    "        # RELU\n",
    "        concatenated_h = F.relu(concatenated_h)\n",
    "        \n",
    "        concatenated_h = self.dropout(concatenated_h)\n",
    "        \n",
    "        output = self.linear(concatenated_h)\n",
    "        \n",
    "        log_probs = F.log_softmax(output)\n",
    "        return log_probs, indices\n",
    "    \n",
    "    @staticmethod\n",
    "    def init_hidden_and_cell_states(batch_size, num_layers, num_directions, hidden_size):\n",
    "        hidden = Variable(torch.randn(num_layers*num_directions, batch_size, hidden_size))\n",
    "        cell = Variable(torch.randn(num_layers*num_directions, batch_size, hidden_size))\n",
    "        \n",
    "        if CUDA_AVAILABLE:\n",
    "            hidden, cell = hidden.cuda(), cell.cuda()\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#model = MyLSTM(len(word_to_ix), EMBEDDING_DIM, LSTM_OUT_DIM, BIDIRECTIONAL, DROPOUT_PROB)\n",
    "#if CUDA_AVAILABLE: model.cuda()\n",
    "#    \n",
    "#for i, batch in enumerate(train_dataset_loader):\n",
    "#    print(i)\n",
    "#    out = model.forward(batch)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Define a few training helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, batch in enumerate(val_loader):\n",
    "\n",
    "        # compute output\n",
    "        \n",
    "        log_probas, indices = model.forward(batch)\n",
    "        labels = Variable(batch['class'][indices])\n",
    "        if CUDA_AVAILABLE: labels = labels.cuda()\n",
    "        loss = loss_fn(log_probas, labels)\n",
    "\n",
    "        losses.update(loss.data[0], len(batch['length']))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % PRINT_FREQUENCY == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "                   i+1, len(val_loader), batch_time=batch_time, loss=losses))\n",
    "    \n",
    "    print(' * Loss {losses.avg:.3f}'.format(losses=losses))\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch, val_loader=None):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        \n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # compute output\n",
    "        model.zero_grad()\n",
    "        log_probas, indices = model.forward(batch)\n",
    "        \n",
    "        labels = Variable(batch['class'][indices])\n",
    "        if CUDA_AVAILABLE: labels = labels.cuda()\n",
    "        \n",
    "        loss = criterion(log_probas, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        losses.update(loss.data[0], len(batch['length']))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % PRINT_FREQUENCY == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "                   epoch, i+1, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses))\n",
    "            \n",
    "        if i % 10 == 0:\n",
    "            validate(val_loader, model, criterion)\n",
    "            model.train()\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename=CHECKPOINT_NAME):\n",
    "    if not os.path.exists(os.path.dirname(CHECKPOINT_NAME)):\n",
    "        os.makedirs(os.path.dirname(CHECKPOINT_NAME))\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, BEST_CHECKPOINT_NAME)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Define model, loss function, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "model = MyLSTM(len(word_to_ix), EMBEDDING_DIM, LSTM_OUT_DIM, BIDIRECTIONAL, DROPOUT_PROB)\n",
    "if CUDA_AVAILABLE:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Start main loop. If checkpoint exists, start from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint 'pytorchckpts/4newarchsmall/checkpoint.pth.tar'\n",
      "=> loaded checkpoint 'pytorchckpts/4newarchsmall/checkpoint.pth.tar' (epoch 4)\n",
      "Epoch: [5][1/83]\tTime 5.429 (5.429)\tData 1.407 (1.407)\tLoss 0.8788 (0.8788)\t\n",
      "Test: [1/15]\tTime 2.079 (2.079)\tLoss 1.1905 (1.1905)\t\n",
      "Test: [2/15]\tTime 2.132 (2.106)\tLoss 1.4684 (1.3294)\t\n",
      "Test: [3/15]\tTime 2.378 (2.196)\tLoss 1.5748 (1.4112)\t\n",
      "Test: [4/15]\tTime 2.484 (2.268)\tLoss 1.4875 (1.4303)\t\n",
      "Test: [5/15]\tTime 2.111 (2.237)\tLoss 1.2381 (1.3919)\t\n",
      "Test: [6/15]\tTime 2.443 (2.271)\tLoss 0.9248 (1.3140)\t\n",
      "Test: [7/15]\tTime 2.123 (2.250)\tLoss 1.1377 (1.2888)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.197 (2.243)\tLoss 1.1806 (1.2753)\t\n",
      "Test: [9/15]\tTime 2.048 (2.222)\tLoss 1.2378 (1.2711)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.159 (2.215)\tLoss 1.1005 (1.2541)\t\n",
      "Test: [11/15]\tTime 2.256 (2.219)\tLoss 1.2305 (1.2519)\t\n",
      "Test: [12/15]\tTime 2.039 (2.204)\tLoss 0.8356 (1.2172)\t\n",
      "Test: [13/15]\tTime 2.276 (2.210)\tLoss 1.5837 (1.2454)\t\n",
      "Test: [14/15]\tTime 2.244 (2.212)\tLoss 1.6967 (1.2777)\t\n",
      "Test: [15/15]\tTime 1.670 (2.176)\tLoss 1.0638 (1.2664)\t\n",
      " * Loss 1.266\n",
      "Epoch: [5][2/83]\tTime 37.908 (21.669)\tData 33.985 (17.696)\tLoss 0.6826 (0.7807)\t\n",
      "Epoch: [5][3/83]\tTime 5.522 (16.287)\tData 1.487 (12.293)\tLoss 0.8098 (0.7904)\t\n",
      "Epoch: [5][4/83]\tTime 5.247 (13.527)\tData 1.342 (9.555)\tLoss 0.8745 (0.8114)\t\n",
      "Epoch: [5][5/83]\tTime 5.332 (11.888)\tData 1.266 (7.898)\tLoss 0.7532 (0.7998)\t\n",
      "Epoch: [5][6/83]\tTime 5.430 (10.811)\tData 1.392 (6.813)\tLoss 0.6965 (0.7826)\t\n",
      "Epoch: [5][7/83]\tTime 5.327 (10.028)\tData 1.294 (6.025)\tLoss 0.4912 (0.7409)\t\n",
      "Epoch: [5][8/83]\tTime 5.562 (9.470)\tData 1.461 (5.454)\tLoss 0.7569 (0.7429)\t\n",
      "Epoch: [5][9/83]\tTime 5.205 (8.996)\tData 1.335 (4.997)\tLoss 0.8332 (0.7530)\t\n",
      "Epoch: [5][10/83]\tTime 5.537 (8.650)\tData 1.514 (4.648)\tLoss 0.3865 (0.7163)\t\n",
      "Epoch: [5][11/83]\tTime 5.246 (8.341)\tData 1.336 (4.347)\tLoss 0.8364 (0.7272)\t\n",
      "Test: [1/15]\tTime 2.105 (2.105)\tLoss 1.2472 (1.2472)\t\n",
      "Test: [2/15]\tTime 2.072 (2.088)\tLoss 1.5179 (1.3826)\t\n",
      "Test: [3/15]\tTime 2.434 (2.204)\tLoss 1.6904 (1.4852)\t\n",
      "Test: [4/15]\tTime 2.507 (2.280)\tLoss 1.5657 (1.5053)\t\n",
      "Test: [5/15]\tTime 2.107 (2.245)\tLoss 1.3488 (1.4740)\t\n",
      "Test: [6/15]\tTime 2.453 (2.280)\tLoss 1.0084 (1.3964)\t\n",
      "Test: [7/15]\tTime 2.146 (2.261)\tLoss 1.2595 (1.3768)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.217 (2.255)\tLoss 1.2961 (1.3667)\t\n",
      "Test: [9/15]\tTime 2.062 (2.234)\tLoss 1.3493 (1.3648)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.193 (2.230)\tLoss 1.0839 (1.3367)\t\n",
      "Test: [11/15]\tTime 2.267 (2.233)\tLoss 1.3201 (1.3352)\t\n",
      "Test: [12/15]\tTime 2.060 (2.219)\tLoss 0.9639 (1.3043)\t\n",
      "Test: [13/15]\tTime 2.320 (2.226)\tLoss 1.6469 (1.3306)\t\n",
      "Test: [14/15]\tTime 2.235 (2.227)\tLoss 1.8529 (1.3679)\t\n",
      "Test: [15/15]\tTime 1.686 (2.191)\tLoss 1.2503 (1.3617)\t\n",
      " * Loss 1.362\n",
      "Epoch: [5][12/83]\tTime 38.281 (10.836)\tData 34.227 (6.837)\tLoss 0.4845 (0.7070)\t\n",
      "Epoch: [5][13/83]\tTime 5.224 (10.404)\tData 1.257 (6.408)\tLoss 0.6794 (0.7049)\t\n",
      "Epoch: [5][14/83]\tTime 5.393 (10.046)\tData 1.368 (6.048)\tLoss 0.8797 (0.7174)\t\n",
      "Epoch: [5][15/83]\tTime 5.234 (9.725)\tData 1.249 (5.728)\tLoss 0.7102 (0.7169)\t\n",
      "Epoch: [5][16/83]\tTime 5.146 (9.439)\tData 1.255 (5.448)\tLoss 1.0462 (0.7375)\t\n",
      "Epoch: [5][17/83]\tTime 5.436 (9.204)\tData 1.341 (5.207)\tLoss 0.7087 (0.7358)\t\n",
      "Epoch: [5][18/83]\tTime 5.253 (8.984)\tData 1.261 (4.988)\tLoss 0.6658 (0.7319)\t\n",
      "Epoch: [5][19/83]\tTime 5.340 (8.792)\tData 1.355 (4.796)\tLoss 0.5778 (0.7238)\t\n",
      "Epoch: [5][20/83]\tTime 5.306 (8.618)\tData 1.314 (4.622)\tLoss 0.6506 (0.7201)\t\n",
      "Epoch: [5][21/83]\tTime 5.305 (8.460)\tData 1.306 (4.464)\tLoss 0.9490 (0.7310)\t\n",
      "Test: [1/15]\tTime 2.102 (2.102)\tLoss 1.2644 (1.2644)\t\n",
      "Test: [2/15]\tTime 2.073 (2.088)\tLoss 1.4405 (1.3524)\t\n",
      "Test: [3/15]\tTime 2.429 (2.201)\tLoss 1.6154 (1.4401)\t\n",
      "Test: [4/15]\tTime 2.512 (2.279)\tLoss 1.5368 (1.4643)\t\n",
      "Test: [5/15]\tTime 2.114 (2.246)\tLoss 1.1528 (1.4020)\t\n",
      "Test: [6/15]\tTime 2.452 (2.280)\tLoss 1.0283 (1.3397)\t\n",
      "Test: [7/15]\tTime 2.146 (2.261)\tLoss 1.2613 (1.3285)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.232 (2.258)\tLoss 1.2102 (1.3137)\t\n",
      "Test: [9/15]\tTime 2.061 (2.236)\tLoss 1.2107 (1.3023)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.178 (2.230)\tLoss 1.1067 (1.2827)\t\n",
      "Test: [11/15]\tTime 2.252 (2.232)\tLoss 1.2669 (1.2813)\t\n",
      "Test: [12/15]\tTime 2.059 (2.218)\tLoss 0.9143 (1.2507)\t\n",
      "Test: [13/15]\tTime 2.279 (2.222)\tLoss 1.5907 (1.2769)\t\n",
      "Test: [14/15]\tTime 2.279 (2.226)\tLoss 1.6145 (1.3010)\t\n",
      "Test: [15/15]\tTime 1.593 (2.184)\tLoss 1.1236 (1.2916)\t\n",
      " * Loss 1.292\n",
      "Epoch: [5][22/83]\tTime 38.172 (9.811)\tData 34.169 (5.815)\tLoss 0.9031 (0.7388)\t\n",
      "Epoch: [5][23/83]\tTime 5.609 (9.628)\tData 1.569 (5.630)\tLoss 0.7726 (0.7403)\t\n",
      "Epoch: [5][24/83]\tTime 5.626 (9.461)\tData 1.585 (5.462)\tLoss 0.7618 (0.7412)\t\n",
      "Epoch: [5][25/83]\tTime 5.402 (9.299)\tData 1.353 (5.297)\tLoss 0.5443 (0.7333)\t\n",
      "Epoch: [5][26/83]\tTime 5.411 (9.149)\tData 1.368 (5.146)\tLoss 0.5197 (0.7251)\t\n",
      "Epoch: [5][27/83]\tTime 5.407 (9.011)\tData 1.357 (5.006)\tLoss 0.8322 (0.7291)\t\n",
      "Epoch: [5][28/83]\tTime 5.366 (8.881)\tData 1.323 (4.874)\tLoss 0.5844 (0.7239)\t\n",
      "Epoch: [5][29/83]\tTime 5.339 (8.759)\tData 1.293 (4.751)\tLoss 0.6181 (0.7203)\t\n",
      "Epoch: [5][30/83]\tTime 5.501 (8.650)\tData 1.502 (4.642)\tLoss 0.6214 (0.7170)\t\n",
      "Epoch: [5][31/83]\tTime 5.421 (8.546)\tData 1.434 (4.539)\tLoss 0.6718 (0.7155)\t\n",
      "Test: [1/15]\tTime 2.098 (2.098)\tLoss 1.3298 (1.3298)\t\n",
      "Test: [2/15]\tTime 2.101 (2.099)\tLoss 1.4648 (1.3973)\t\n",
      "Test: [3/15]\tTime 2.446 (2.215)\tLoss 1.6085 (1.4677)\t\n",
      "Test: [4/15]\tTime 2.415 (2.265)\tLoss 1.5897 (1.4982)\t\n",
      "Test: [5/15]\tTime 2.176 (2.247)\tLoss 1.1405 (1.4267)\t\n",
      "Test: [6/15]\tTime 2.370 (2.268)\tLoss 1.0470 (1.3634)\t\n",
      "Test: [7/15]\tTime 2.190 (2.256)\tLoss 1.2687 (1.3499)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.205 (2.250)\tLoss 1.2233 (1.3340)\t\n",
      "Test: [9/15]\tTime 2.057 (2.229)\tLoss 1.2611 (1.3259)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.163 (2.222)\tLoss 1.1001 (1.3034)\t\n",
      "Test: [11/15]\tTime 2.239 (2.224)\tLoss 1.3252 (1.3053)\t\n",
      "Test: [12/15]\tTime 2.044 (2.209)\tLoss 0.8520 (1.2676)\t\n",
      "Test: [13/15]\tTime 2.300 (2.216)\tLoss 1.6177 (1.2945)\t\n",
      "Test: [14/15]\tTime 2.244 (2.218)\tLoss 1.6812 (1.3221)\t\n",
      "Test: [15/15]\tTime 1.605 (2.177)\tLoss 1.1861 (1.3150)\t\n",
      " * Loss 1.315\n",
      "Epoch: [5][32/83]\tTime 38.240 (9.474)\tData 34.132 (5.464)\tLoss 0.8325 (0.7192)\t\n",
      "Epoch: [5][33/83]\tTime 5.171 (9.343)\tData 1.191 (5.334)\tLoss 0.7764 (0.7209)\t\n",
      "Epoch: [5][34/83]\tTime 5.359 (9.226)\tData 1.455 (5.220)\tLoss 0.8895 (0.7259)\t\n",
      "Epoch: [5][35/83]\tTime 5.539 (9.121)\tData 1.495 (5.114)\tLoss 0.8377 (0.7291)\t\n",
      "Epoch: [5][36/83]\tTime 5.375 (9.017)\tData 1.343 (5.009)\tLoss 1.1642 (0.7411)\t\n",
      "Epoch: [5][37/83]\tTime 5.380 (8.919)\tData 1.325 (4.909)\tLoss 0.6072 (0.7375)\t\n",
      "Epoch: [5][38/83]\tTime 5.344 (8.824)\tData 1.450 (4.818)\tLoss 0.5303 (0.7321)\t\n",
      "Epoch: [5][39/83]\tTime 5.391 (8.736)\tData 1.352 (4.729)\tLoss 0.4999 (0.7261)\t\n",
      "Epoch: [5][40/83]\tTime 5.575 (8.657)\tData 1.476 (4.648)\tLoss 0.9627 (0.7320)\t\n",
      "Epoch: [5][41/83]\tTime 5.298 (8.575)\tData 1.342 (4.567)\tLoss 0.6102 (0.7291)\t\n",
      "Test: [1/15]\tTime 2.102 (2.102)\tLoss 1.3824 (1.3824)\t\n",
      "Test: [2/15]\tTime 2.084 (2.093)\tLoss 1.4783 (1.4304)\t\n",
      "Test: [3/15]\tTime 2.426 (2.204)\tLoss 1.5460 (1.4689)\t\n",
      "Test: [4/15]\tTime 2.419 (2.258)\tLoss 1.4951 (1.4755)\t\n",
      "Test: [5/15]\tTime 2.189 (2.244)\tLoss 1.2674 (1.4338)\t\n",
      "Test: [6/15]\tTime 2.380 (2.267)\tLoss 1.0254 (1.3658)\t\n",
      "Test: [7/15]\tTime 2.180 (2.254)\tLoss 1.3336 (1.3612)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.142 (2.240)\tLoss 1.3518 (1.3600)\t\n",
      "Test: [9/15]\tTime 2.107 (2.226)\tLoss 1.3258 (1.3562)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.158 (2.219)\tLoss 1.0218 (1.3228)\t\n",
      "Test: [11/15]\tTime 2.239 (2.221)\tLoss 1.3157 (1.3221)\t\n",
      "Test: [12/15]\tTime 2.037 (2.205)\tLoss 0.9877 (1.2942)\t\n",
      "Test: [13/15]\tTime 2.282 (2.211)\tLoss 1.6887 (1.3246)\t\n",
      "Test: [14/15]\tTime 2.248 (2.214)\tLoss 1.7121 (1.3523)\t\n",
      "Test: [15/15]\tTime 1.600 (2.173)\tLoss 1.2103 (1.3448)\t\n",
      " * Loss 1.345\n",
      "Epoch: [5][42/83]\tTime 37.758 (9.270)\tData 33.796 (5.263)\tLoss 0.5538 (0.7249)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Epoch: [5][43/83]\tTime 5.299 (9.178)\tData 1.374 (5.173)\tLoss 0.8093 (0.7269)\t\n",
      "Epoch: [5][44/83]\tTime 5.622 (9.097)\tData 1.533 (5.090)\tLoss 0.8518 (0.7297)\t\n",
      "Epoch: [5][45/83]\tTime 5.260 (9.012)\tData 1.259 (5.005)\tLoss 0.7002 (0.7290)\t\n",
      "Epoch: [5][46/83]\tTime 5.353 (8.932)\tData 1.349 (4.926)\tLoss 0.6383 (0.7271)\t\n",
      "Epoch: [5][47/83]\tTime 5.206 (8.853)\tData 1.255 (4.848)\tLoss 0.8578 (0.7298)\t\n",
      "Epoch: [5][48/83]\tTime 5.457 (8.782)\tData 1.451 (4.777)\tLoss 0.5231 (0.7255)\t\n",
      "Epoch: [5][49/83]\tTime 5.446 (8.714)\tData 1.354 (4.707)\tLoss 0.7793 (0.7266)\t\n",
      "Epoch: [5][50/83]\tTime 5.479 (8.649)\tData 1.484 (4.642)\tLoss 0.6574 (0.7252)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Epoch: [5][51/83]\tTime 5.339 (8.585)\tData 1.462 (4.580)\tLoss 0.6578 (0.7239)\t\n",
      "Test: [1/15]\tTime 2.105 (2.105)\tLoss 1.4134 (1.4134)\t\n",
      "Test: [2/15]\tTime 2.074 (2.090)\tLoss 1.5039 (1.4587)\t\n",
      "Test: [3/15]\tTime 2.451 (2.210)\tLoss 1.5184 (1.4786)\t\n",
      "Test: [4/15]\tTime 2.414 (2.261)\tLoss 1.4253 (1.4652)\t\n",
      "Test: [5/15]\tTime 2.176 (2.244)\tLoss 1.3227 (1.4367)\t\n",
      "Test: [6/15]\tTime 2.379 (2.267)\tLoss 1.0902 (1.3790)\t\n",
      "Test: [7/15]\tTime 2.190 (2.256)\tLoss 1.3229 (1.3710)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.132 (2.240)\tLoss 1.4150 (1.3765)\t\n",
      "Test: [9/15]\tTime 2.111 (2.226)\tLoss 1.3762 (1.3764)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.150 (2.218)\tLoss 1.0105 (1.3399)\t\n",
      "Test: [11/15]\tTime 2.234 (2.220)\tLoss 1.3142 (1.3375)\t\n",
      "Test: [12/15]\tTime 2.083 (2.208)\tLoss 1.0059 (1.3099)\t\n",
      "Test: [13/15]\tTime 2.302 (2.216)\tLoss 1.6572 (1.3366)\t\n",
      "Test: [14/15]\tTime 2.278 (2.220)\tLoss 1.6048 (1.3558)\t\n",
      "Test: [15/15]\tTime 1.617 (2.180)\tLoss 1.2594 (1.3507)\t\n",
      " * Loss 1.351\n",
      "Epoch: [5][52/83]\tTime 38.110 (9.152)\tData 33.995 (5.146)\tLoss 0.6009 (0.7216)\t\n",
      "Epoch: [5][53/83]\tTime 5.300 (9.080)\tData 1.355 (5.074)\tLoss 0.8338 (0.7237)\t\n",
      "Epoch: [5][54/83]\tTime 5.454 (9.013)\tData 1.403 (5.006)\tLoss 0.6816 (0.7229)\t\n",
      "Epoch: [5][55/83]\tTime 5.267 (8.944)\tData 1.304 (4.939)\tLoss 0.6734 (0.7220)\t\n",
      "Epoch: [5][56/83]\tTime 5.435 (8.882)\tData 1.441 (4.876)\tLoss 1.0555 (0.7280)\t\n",
      "Epoch: [5][57/83]\tTime 5.432 (8.821)\tData 1.333 (4.814)\tLoss 0.7801 (0.7289)\t\n",
      "Epoch: [5][58/83]\tTime 5.280 (8.760)\tData 1.297 (4.754)\tLoss 0.6345 (0.7272)\t\n",
      "Epoch: [5][59/83]\tTime 5.696 (8.708)\tData 1.596 (4.700)\tLoss 0.7130 (0.7270)\t\n",
      "Epoch: [5][60/83]\tTime 5.382 (8.653)\tData 1.405 (4.645)\tLoss 0.8003 (0.7282)\t\n",
      "Epoch: [5][61/83]\tTime 5.420 (8.600)\tData 1.317 (4.591)\tLoss 0.8165 (0.7297)\t\n",
      "Test: [1/15]\tTime 2.094 (2.094)\tLoss 1.3280 (1.3280)\t\n",
      "Test: [2/15]\tTime 2.064 (2.079)\tLoss 1.5105 (1.4193)\t\n",
      "Test: [3/15]\tTime 2.463 (2.207)\tLoss 1.7148 (1.5178)\t\n",
      "Test: [4/15]\tTime 2.423 (2.261)\tLoss 1.4966 (1.5125)\t\n",
      "Test: [5/15]\tTime 2.179 (2.245)\tLoss 1.2721 (1.4644)\t\n",
      "Test: [6/15]\tTime 2.368 (2.265)\tLoss 0.9956 (1.3863)\t\n",
      "Test: [7/15]\tTime 2.181 (2.253)\tLoss 1.2311 (1.3641)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.129 (2.238)\tLoss 1.4438 (1.3741)\t\n",
      "Test: [9/15]\tTime 2.127 (2.225)\tLoss 1.3134 (1.3673)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.086 (2.211)\tLoss 1.1312 (1.3437)\t\n",
      "Test: [11/15]\tTime 2.301 (2.220)\tLoss 1.3546 (1.3447)\t\n",
      "Test: [12/15]\tTime 2.062 (2.206)\tLoss 0.9571 (1.3124)\t\n",
      "Test: [13/15]\tTime 2.287 (2.213)\tLoss 1.7672 (1.3474)\t\n",
      "Test: [14/15]\tTime 2.232 (2.214)\tLoss 1.6805 (1.3712)\t\n",
      "Test: [15/15]\tTime 1.607 (2.174)\tLoss 1.2622 (1.3655)\t\n",
      " * Loss 1.365\n",
      "Epoch: [5][62/83]\tTime 38.075 (9.075)\tData 34.031 (5.066)\tLoss 0.9530 (0.7333)\t\n",
      "Epoch: [5][63/83]\tTime 5.286 (9.015)\tData 1.248 (5.005)\tLoss 1.5847 (0.7468)\t\n",
      "Epoch: [5][64/83]\tTime 5.290 (8.957)\tData 1.241 (4.946)\tLoss 0.6601 (0.7454)\t\n",
      "Epoch: [5][65/83]\tTime 5.566 (8.905)\tData 1.524 (4.893)\tLoss 0.6183 (0.7435)\t\n",
      "Epoch: [5][66/83]\tTime 5.465 (8.853)\tData 1.471 (4.842)\tLoss 0.8453 (0.7450)\t\n",
      "Epoch: [5][67/83]\tTime 5.249 (8.799)\tData 1.287 (4.789)\tLoss 0.8210 (0.7462)\t\n",
      "Epoch: [5][68/83]\tTime 5.391 (8.749)\tData 1.390 (4.739)\tLoss 0.5590 (0.7434)\t\n",
      "Epoch: [5][69/83]\tTime 5.549 (8.702)\tData 1.450 (4.691)\tLoss 0.8746 (0.7453)\t\n",
      "Epoch: [5][70/83]\tTime 5.260 (8.653)\tData 1.256 (4.642)\tLoss 0.4881 (0.7416)\t\n",
      "Epoch: [5][71/83]\tTime 5.417 (8.608)\tData 1.439 (4.597)\tLoss 0.7016 (0.7411)\t\n",
      "Test: [1/15]\tTime 2.041 (2.041)\tLoss 1.2604 (1.2604)\t\n",
      "Test: [2/15]\tTime 2.147 (2.094)\tLoss 1.4978 (1.3791)\t\n",
      "Test: [3/15]\tTime 2.467 (2.218)\tLoss 1.6917 (1.4833)\t\n",
      "Test: [4/15]\tTime 2.434 (2.272)\tLoss 1.6147 (1.5161)\t\n",
      "Test: [5/15]\tTime 2.186 (2.255)\tLoss 1.2323 (1.4594)\t\n",
      "Test: [6/15]\tTime 2.375 (2.275)\tLoss 1.0102 (1.3845)\t\n",
      "Test: [7/15]\tTime 2.183 (2.262)\tLoss 1.2013 (1.3583)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.156 (2.249)\tLoss 1.3913 (1.3625)\t\n",
      "Test: [9/15]\tTime 2.116 (2.234)\tLoss 1.2423 (1.3491)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.087 (2.219)\tLoss 1.1060 (1.3248)\t\n",
      "Test: [11/15]\tTime 2.318 (2.228)\tLoss 1.3313 (1.3254)\t\n",
      "Test: [12/15]\tTime 1.986 (2.208)\tLoss 0.8925 (1.2893)\t\n",
      "Test: [13/15]\tTime 2.345 (2.218)\tLoss 1.6524 (1.3172)\t\n",
      "Test: [14/15]\tTime 2.247 (2.221)\tLoss 1.5175 (1.3315)\t\n",
      "Test: [15/15]\tTime 1.624 (2.181)\tLoss 1.2192 (1.3256)\t\n",
      " * Loss 1.326\n",
      "Epoch: [5][72/83]\tTime 38.242 (9.019)\tData 34.127 (5.007)\tLoss 0.5490 (0.7384)\t\n",
      "Epoch: [5][73/83]\tTime 5.311 (8.968)\tData 1.275 (4.956)\tLoss 0.5587 (0.7359)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Epoch: [5][74/83]\tTime 5.324 (8.919)\tData 1.315 (4.907)\tLoss 1.0175 (0.7397)\t\n",
      "Epoch: [5][75/83]\tTime 5.407 (8.872)\tData 1.498 (4.861)\tLoss 0.6231 (0.7382)\t\n",
      "Epoch: [5][76/83]\tTime 5.284 (8.825)\tData 1.300 (4.814)\tLoss 0.9630 (0.7411)\t\n",
      "Epoch: [5][77/83]\tTime 5.560 (8.783)\tData 1.457 (4.771)\tLoss 0.7185 (0.7408)\t\n",
      "Epoch: [5][78/83]\tTime 5.291 (8.738)\tData 1.264 (4.726)\tLoss 1.0359 (0.7446)\t\n",
      "Epoch: [5][79/83]\tTime 5.360 (8.695)\tData 1.314 (4.683)\tLoss 0.8219 (0.7456)\t\n",
      "Epoch: [5][80/83]\tTime 5.321 (8.653)\tData 1.280 (4.640)\tLoss 0.7931 (0.7462)\t\n",
      "Epoch: [5][81/83]\tTime 5.515 (8.614)\tData 1.414 (4.600)\tLoss 0.9334 (0.7485)\t\n",
      "Test: [1/15]\tTime 2.026 (2.026)\tLoss 1.5352 (1.5352)\t\n",
      "Test: [2/15]\tTime 2.129 (2.078)\tLoss 1.6848 (1.6100)\t\n",
      "Test: [3/15]\tTime 2.452 (2.202)\tLoss 1.8324 (1.6841)\t\n",
      "Test: [4/15]\tTime 2.436 (2.261)\tLoss 1.7998 (1.7130)\t\n",
      "Test: [5/15]\tTime 2.194 (2.247)\tLoss 1.4722 (1.6649)\t\n",
      "Test: [6/15]\tTime 2.377 (2.269)\tLoss 1.2161 (1.5901)\t\n",
      "Test: [7/15]\tTime 2.182 (2.257)\tLoss 1.4846 (1.5750)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.142 (2.242)\tLoss 1.5464 (1.5714)\t\n",
      "Test: [9/15]\tTime 2.109 (2.227)\tLoss 1.6179 (1.5766)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.093 (2.214)\tLoss 1.0982 (1.5288)\t\n",
      "Test: [11/15]\tTime 2.297 (2.222)\tLoss 1.5200 (1.5280)\t\n",
      "Test: [12/15]\tTime 1.964 (2.200)\tLoss 1.0586 (1.4889)\t\n",
      "Test: [13/15]\tTime 2.348 (2.211)\tLoss 2.0367 (1.5310)\t\n",
      "Test: [14/15]\tTime 2.156 (2.207)\tLoss 1.7944 (1.5498)\t\n",
      "Test: [15/15]\tTime 1.662 (2.171)\tLoss 1.4793 (1.5461)\t\n",
      " * Loss 1.546\n",
      "Epoch: [5][82/83]\tTime 37.967 (8.972)\tData 33.850 (4.957)\tLoss 0.8772 (0.7501)\t\n",
      "Epoch: [5][83/83]\tTime 5.377 (8.929)\tData 1.483 (4.915)\tLoss 1.1733 (0.7552)\t\n",
      "Test: [1/15]\tTime 2.077 (2.077)\tLoss 1.5189 (1.5189)\t\n",
      "Test: [2/15]\tTime 2.049 (2.063)\tLoss 1.7053 (1.6121)\t\n",
      "Test: [3/15]\tTime 2.375 (2.167)\tLoss 1.8163 (1.6802)\t\n",
      "Test: [4/15]\tTime 2.387 (2.222)\tLoss 1.7589 (1.6998)\t\n",
      "Test: [5/15]\tTime 2.115 (2.200)\tLoss 1.4861 (1.6571)\t\n",
      "Test: [6/15]\tTime 2.325 (2.221)\tLoss 1.1686 (1.5757)\t\n",
      "Test: [7/15]\tTime 2.133 (2.209)\tLoss 1.4471 (1.5573)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.176 (2.205)\tLoss 1.5102 (1.5514)\t\n",
      "Test: [9/15]\tTime 2.042 (2.186)\tLoss 1.5103 (1.5468)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.144 (2.182)\tLoss 1.1501 (1.5072)\t\n",
      "Test: [11/15]\tTime 2.225 (2.186)\tLoss 1.5021 (1.5067)\t\n",
      "Test: [12/15]\tTime 1.996 (2.170)\tLoss 1.0318 (1.4671)\t\n",
      "Test: [13/15]\tTime 2.239 (2.176)\tLoss 1.9957 (1.5078)\t\n",
      "Test: [14/15]\tTime 2.195 (2.177)\tLoss 1.8384 (1.5314)\t\n",
      "Test: [15/15]\tTime 1.576 (2.137)\tLoss 1.4864 (1.5290)\t\n",
      " * Loss 1.529\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(CHECKPOINT_NAME):\n",
    "    print(\"=> loading checkpoint '{}'\".format(CHECKPOINT_NAME))\n",
    "    checkpoint = torch.load(CHECKPOINT_NAME)\n",
    "    EPOCH = checkpoint['epoch']\n",
    "    BEST_LOSS = checkpoint['best_loss']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "          .format(CHECKPOINT_NAME, checkpoint['epoch']))\n",
    "else:\n",
    "    print(\"=> no checkpoint found at '{}'. Starting from scratch\".format(CHECKPOINT_NAME))\n",
    "    \n",
    "for epoch in range(EPOCH, NUM_EPOCHS):\n",
    "    train(train_dataset_loader, model, loss_fn, optimizer, epoch + 1, val_dataset_loader)\n",
    "    loss = validate(val_dataset_loader, model, loss_fn)\n",
    "    \n",
    "    if loss < BEST_LOSS:\n",
    "        print('{} better than previous best loss of {}'.format(loss, BEST_LOSS))\n",
    "        BEST_LOSS = loss\n",
    "        is_best = True\n",
    "    else:\n",
    "        is_best = False\n",
    "    \n",
    "    save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_loss': BEST_LOSS,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4554716895397444"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Generate submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint 'pytorchckpts/4newarchsmall/model_best.pth.tar'\n",
      "=> loaded checkpoint 'pytorchckpts/4newarchsmall/model_best.pth.tar' (epoch 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MyLSTM (\n",
       "  (embedding): Embedding(4588, 64)\n",
       "  (lstm): LSTM(66, 64, bidirectional=True)\n",
       "  (linear): Linear (256 -> 9)\n",
       "  (dropout): Dropout (p = 0.5)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists(CHECKPOINT_NAME):\n",
    "    print(\"=> loading checkpoint '{}'\".format(BEST_CHECKPOINT_NAME))\n",
    "    checkpoint = torch.load(BEST_CHECKPOINT_NAME)\n",
    "    EPOCH = checkpoint['epoch']\n",
    "    BEST_LOSS = checkpoint['best_loss']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "          .format(BEST_CHECKPOINT_NAME, checkpoint['epoch']))\n",
    "else:\n",
    "    raise Exception(\"=> no checkpoint found at '{}'. Cannot generate submission\".format(BEST_CHECKPOINT_NAME))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "FOUND NULL SENTENCE!!!\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "test_dataset = SentencesDataset(df_test, word_to_ix, word_limit=TEXT_WORD_LIMIT)\n",
    "test_dataset_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "probas = []\n",
    "for i, test_batch in enumerate(test_dataset_loader):\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "    log_probas, indices = model.forward(test_batch)\n",
    "    _, orig_indices = torch.sort(indices)\n",
    "    log_probas = log_probas.data.cpu()[orig_indices]\n",
    "    probas.append(log_probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.0622  0.1158  0.0011  ...   0.7752  0.0004  0.0012\n",
       " 0.1170  0.0347  0.0009  ...   0.7903  0.0005  0.0008\n",
       " 0.2793  0.0555  0.0008  ...   0.3545  0.0007  0.0016\n",
       "          ...             â‹±             ...          \n",
       " 0.1399  0.0802  0.0005  ...   0.6757  0.0009  0.0022\n",
       " 0.2980  0.0875  0.0012  ...   0.4194  0.0013  0.0015\n",
       " 0.1367  0.1685  0.0023  ...   0.5363  0.0006  0.0057\n",
       "[torch.FloatTensor of size 5668x9]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas = torch.exp(torch.cat(probas, dim=0))\n",
    "probas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6.21822216e-02,   1.15822256e-01,   1.07325823e-03,\n",
       "         2.51004081e-02,   1.43704694e-02,   4.66867723e-03,\n",
       "         7.75180221e-01,   3.96610150e-04,   1.20594550e-03], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 3, 4, 5, 6, 8]),\n",
       " array([ 212,   23,  134,   19,   27, 5252,    1]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.argmax(probas.numpy(), axis=1), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class1</th>\n",
       "      <th>class2</th>\n",
       "      <th>class3</th>\n",
       "      <th>class4</th>\n",
       "      <th>class5</th>\n",
       "      <th>class6</th>\n",
       "      <th>class7</th>\n",
       "      <th>class8</th>\n",
       "      <th>class9</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.062182</td>\n",
       "      <td>0.115822</td>\n",
       "      <td>0.001073</td>\n",
       "      <td>0.025100</td>\n",
       "      <td>0.014370</td>\n",
       "      <td>0.004669</td>\n",
       "      <td>0.775180</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.116994</td>\n",
       "      <td>0.034654</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.023114</td>\n",
       "      <td>0.028921</td>\n",
       "      <td>0.003864</td>\n",
       "      <td>0.790295</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.279304</td>\n",
       "      <td>0.055468</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>0.217349</td>\n",
       "      <td>0.075128</td>\n",
       "      <td>0.015165</td>\n",
       "      <td>0.354532</td>\n",
       "      <td>0.000658</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.270914</td>\n",
       "      <td>0.126977</td>\n",
       "      <td>0.001761</td>\n",
       "      <td>0.088126</td>\n",
       "      <td>0.066954</td>\n",
       "      <td>0.017781</td>\n",
       "      <td>0.419938</td>\n",
       "      <td>0.004516</td>\n",
       "      <td>0.003032</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.325930</td>\n",
       "      <td>0.068644</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.095630</td>\n",
       "      <td>0.081464</td>\n",
       "      <td>0.039915</td>\n",
       "      <td>0.381639</td>\n",
       "      <td>0.001762</td>\n",
       "      <td>0.003535</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     class1    class2    class3    class4    class5    class6    class7  \\\n",
       "0  0.062182  0.115822  0.001073  0.025100  0.014370  0.004669  0.775180   \n",
       "1  0.116994  0.034654  0.000936  0.023114  0.028921  0.003864  0.790295   \n",
       "2  0.279304  0.055468  0.000751  0.217349  0.075128  0.015165  0.354532   \n",
       "3  0.270914  0.126977  0.001761  0.088126  0.066954  0.017781  0.419938   \n",
       "4  0.325930  0.068644  0.001481  0.095630  0.081464  0.039915  0.381639   \n",
       "\n",
       "     class8    class9  ID  \n",
       "0  0.000397  0.001206   0  \n",
       "1  0.000465  0.000756   1  \n",
       "2  0.000658  0.001646   2  \n",
       "3  0.004516  0.003032   3  \n",
       "4  0.001762  0.003535   4  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df = pd.DataFrame(probas.numpy(), columns=['class'+str(c+1) for c in range(9)])\n",
    "submission_df['ID'] = df_test['ID']\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "py3.5",
   "language": "python",
   "name": "python35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
