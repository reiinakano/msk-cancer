{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import os \n",
    "import time\n",
    "import joblib\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Define adjustable parameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "EMBEDDING_DIM = 32\n",
    "LSTM_OUT_DIM = 64\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT_PROB = 0.5\n",
    "\n",
    "TOKENIZER_TOP_N_WORDS = 2000  # Increasing this would increase embedding size\n",
    "TEXT_WORD_LIMIT = 3000\n",
    "\n",
    "CUDA_AVAILABLE = torch.cuda.is_available()\n",
    "\n",
    "PRINT_FREQUENCY = 1\n",
    "\n",
    "CHECKPOINT_FOLDER = 'pytorchckpts/3smallnet'\n",
    "\n",
    "NUM_EPOCHS = 3  # You can change this to number of epochs you want the model to go through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do not change unless you know what this does. No need to change this even when resuming\n",
    "BEST_LOSS = np.inf \n",
    "EPOCH = 0 \n",
    "CHECKPOINT_NAME = os.path.join(CHECKPOINT_FOLDER, 'checkpoint.pth.tar')\n",
    "BEST_CHECKPOINT_NAME = os.path.join(CHECKPOINT_FOLDER, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load and merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_train_txt = pd.read_csv('training_text', sep='\\|\\|', header=None, skiprows=1, names=[\"ID\",\"Text\"], engine='python')\n",
    "df_train_var = pd.read_csv('training_variants')\n",
    "df_test_txt = pd.read_csv('test_text', sep='\\|\\|', header=None, skiprows=1, names=[\"ID\",\"Text\"], engine='python')\n",
    "df_test_var = pd.read_csv('test_variants')\n",
    "df_train = pd.merge(df_train_var, df_train_txt, how='left', on='ID')\n",
    "df_test = pd.merge(df_test_var, df_test_txt, how='left', on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Split off a validation set\n",
    "df_train, df_val = train_test_split(df_train, test_size = 0.2, random_state = 42, stratify=df_train['Class'].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class SentencesDataset(Dataset):\n",
    "    def __init__(self, df, word_to_ix, word_limit=2000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: pandas dataframe of same format as df_train/df_test\n",
    "            \n",
    "            word_to_ix: word to index dictionary\n",
    "            \n",
    "            word_limit: Number of words to limit\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.word_to_ix = word_to_ix\n",
    "        self.word_limit = word_limit\n",
    "        if 'Class' in df:\n",
    "            self.le = LabelEncoder().fit(df['Class'].values)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df['Text'].values[idx]\n",
    "        gene = self.df['Gene'].values[idx]\n",
    "        variation = self.df['Variation'].values[idx]\n",
    "        \n",
    "        # Tokenize text. If word count becomes greater than limit, break.\n",
    "        num_words = 0\n",
    "        encoded_tokenized_text = []\n",
    "        for sent in nltk.sent_tokenize(text):\n",
    "            for word in nltk.word_tokenize(sent):\n",
    "                if word in self.word_to_ix:\n",
    "                    encoded_tokenized_text.append(self.word_to_ix[word])\n",
    "                    num_words += 1\n",
    "                    if num_words >= self.word_limit: break\n",
    "            if num_words >= self.word_limit: break\n",
    "        \n",
    "        # Special case: number of tokenized words = 0. Change to single word of unknown\n",
    "        if num_words == 0:\n",
    "            encoded_tokenized_text = [self.word_to_ix['Unknown']]\n",
    "            print('FOUND NULL SENTENCE!!!')\n",
    "            num_words += 1\n",
    "        \n",
    "        # Pad tokenized text if needed\n",
    "        if num_words < self.word_limit:\n",
    "            encoded_tokenized_text += [0] * (self.word_limit - len(encoded_tokenized_text))\n",
    "        \n",
    "        # Create sample\n",
    "        sample = {'text': encoded_tokenized_text,\n",
    "                  'length': num_words,\n",
    "                  'gene': self.word_to_ix[gene] if gene in self.word_to_ix else self.word_to_ix['Unknown'],\n",
    "                  'variation': self.word_to_ix[variation] if variation in self.word_to_ix else self.word_to_ix['Unknown']}\n",
    "        \n",
    "        # If contains class, include class in sample\n",
    "        if 'Class' in self.df:\n",
    "            sample['class'] =  self.le.transform([self.df['Class'].values[idx]])[0]\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build word to index dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_word_to_ix(df_train, location, top_n_words=TOKENIZER_TOP_N_WORDS):\n",
    "    \"\"\"Builds word_to_ix dictionary and saves it in location\"\"\"\n",
    "    if os.path.exists(location):\n",
    "        print('found pickled word_to_ix')\n",
    "        return joblib.load(location)\n",
    "    \n",
    "    word_counts = dict()\n",
    "\n",
    "    for doc in df_train['Text'].values:\n",
    "        for sent in nltk.sent_tokenize(doc):\n",
    "            for word in nltk.word_tokenize(sent):\n",
    "                if word not in word_counts:\n",
    "                    word_counts[word] = 1\n",
    "                else:\n",
    "                    word_counts[word] += 1\n",
    "    \n",
    "    wcounts = list(word_counts.items())\n",
    "    wcounts.sort(key=lambda x: x[1], reverse=True)\n",
    "    sorted_voc = [wc[0] for wc in wcounts]\n",
    "    word_to_ix = dict(list(zip(sorted_voc, list(range(top_n_words)))))\n",
    "    \n",
    "    ix = len(word_to_ix)\n",
    "\n",
    "    for gene in df_train['Gene'].values:\n",
    "        if gene not in word_to_ix:\n",
    "            word_to_ix[gene] = ix\n",
    "            ix += 1\n",
    "\n",
    "    for variation in df_train['Variation'].values:\n",
    "        if variation not in word_to_ix:\n",
    "            word_to_ix[variation] = ix\n",
    "            ix += 1\n",
    "    \n",
    "    if 'Unknown' not in word_to_ix:\n",
    "        word_to_ix['Unknown'] = ix\n",
    "    \n",
    "    joblib.dump(word_to_ix, location, compress=3)\n",
    "    \n",
    "    return word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found pickled word_to_ix\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = build_word_to_ix(df_train, 'word_to_ix_3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_dataset = SentencesDataset(df_train, word_to_ix, word_limit=TEXT_WORD_LIMIT)\n",
    "train_dataset_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_dataset = SentencesDataset(df_val, word_to_ix, word_limit=TEXT_WORD_LIMIT)\n",
    "val_dataset_loader = DataLoader(val_dataset, batch_size=45, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_dataset_loader):\n",
    "    print(i)\n",
    "    if i > 1: \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_out_dim, bidirectional, prob):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm_out_dim = lstm_out_dim\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim*3, lstm_out_dim, bidirectional=bidirectional)\n",
    "        self.linear = nn.Linear(lstm_out_dim*self.num_directions, 9)\n",
    "        self.dropout = nn.Dropout(p=prob)  \n",
    "        \n",
    "    def forward(self, batch):\n",
    "        batch_size = len(batch['length'])\n",
    "        \n",
    "        # Sort all tensors by length of the sequence in decreasing order (for packed padded sequences to work)\n",
    "        length, indices = torch.sort(batch['length'], dim=0, descending=True)\n",
    "        gene = batch['gene'][indices]\n",
    "        variation = batch['variation'][indices]\n",
    "        text_batch = torch.stack(batch['text'], 0)[:, indices]\n",
    "        \n",
    "        # Wrap all tensors around a variable. Send to GPU if possible.\n",
    "        text_batch = Variable(text_batch)\n",
    "        length = Variable(length)\n",
    "        gene = Variable(gene)\n",
    "        variation = Variable(variation)\n",
    "        if CUDA_AVAILABLE:\n",
    "            text_batch, length, gene, variation = text_batch.cuda(), length.cuda(), gene.cuda(), variation.cuda()\n",
    "        \n",
    "        # Pass text, gene, and variation to embedding\n",
    "        embedded_text = self.embedding(text_batch)\n",
    "        embedded_gene = self.embedding(gene)\n",
    "        embedded_variation = self.embedding(variation)\n",
    "        \n",
    "        # Concatenate gene + variation with embedded text\n",
    "        concatenated_embedded = torch.cat([embedded_text,\n",
    "                                           torch.unsqueeze(embedded_gene, dim=0).expand(\n",
    "                                               text_batch.data.shape[0], batch_size, self.embedding_dim),\n",
    "                                           torch.unsqueeze(embedded_variation, dim=0).expand(\n",
    "                                               text_batch.data.shape[0], batch_size, self.embedding_dim)\n",
    "                                          ],\n",
    "                                          dim=2)\n",
    "        \n",
    "        # Initialize hidden and cell states for LSTM\n",
    "        h0, c0 = self.init_hidden_and_cell_states(batch_size, 1, self.num_directions, self.lstm_out_dim)\n",
    "        \n",
    "        # Pack sequence, run through LSTM, and unpack output\n",
    "        packed_embedded_text = pack_padded_sequence(concatenated_embedded, list(length.data))\n",
    "        packed_h, (packed_h_t, packed_c_t) = self.lstm(packed_embedded_text, (h0, c0))\n",
    "        h, _ = pad_packed_sequence(packed_h, batch_first=True)\n",
    "        \n",
    "        # Use fancy indexing to retrieve LSTM outputs for last timestep in each sequence\n",
    "        h = h[\n",
    "            np.arange(batch_size).reshape(-1, 1).tolist(), \n",
    "            length.data.view(-1, 1) - 1, \n",
    "            list(range(self.lstm_out_dim*self.num_directions))\n",
    "        ]\n",
    "        \n",
    "        # RELU\n",
    "        h = F.relu(h)\n",
    "        \n",
    "        h = self.dropout(h)\n",
    "        \n",
    "        output = self.linear(h)\n",
    "        \n",
    "        log_probs = F.log_softmax(output)\n",
    "        return log_probs, indices\n",
    "    \n",
    "    @staticmethod\n",
    "    def init_hidden_and_cell_states(batch_size, num_layers, num_directions, hidden_size):\n",
    "        hidden = Variable(torch.randn(num_layers*num_directions, batch_size, hidden_size))\n",
    "        cell = Variable(torch.randn(num_layers*num_directions, batch_size, hidden_size))\n",
    "        \n",
    "        if CUDA_AVAILABLE:\n",
    "            hidden, cell = hidden.cuda(), cell.cuda()\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Define a few training helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, batch in enumerate(val_loader):\n",
    "\n",
    "        # compute output\n",
    "        \n",
    "        log_probas, indices = model.forward(batch)\n",
    "        labels = Variable(batch['class'][indices])\n",
    "        if CUDA_AVAILABLE: labels = labels.cuda()\n",
    "        loss = loss_fn(log_probas, labels)\n",
    "\n",
    "        losses.update(loss.data[0], len(batch['length']))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % PRINT_FREQUENCY == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "                   i+1, len(val_loader), batch_time=batch_time, loss=losses))\n",
    "    \n",
    "    print(' * Loss {losses.avg:.3f}'.format(losses=losses))\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch, val_loader=None):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        \n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # compute output\n",
    "        model.zero_grad()\n",
    "        log_probas, indices = model.forward(batch)\n",
    "        \n",
    "        labels = Variable(batch['class'][indices])\n",
    "        if CUDA_AVAILABLE: labels = labels.cuda()\n",
    "        \n",
    "        loss = criterion(log_probas, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        losses.update(loss.data[0], len(batch['length']))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % PRINT_FREQUENCY == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "                   epoch, i+1, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses))\n",
    "            \n",
    "        if i % 10 == 0:\n",
    "            validate(val_loader, model, criterion)\n",
    "            model.train()\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename=CHECKPOINT_NAME):\n",
    "    if not os.path.exists(os.path.dirname(CHECKPOINT_NAME)):\n",
    "        os.makedirs(os.path.dirname(CHECKPOINT_NAME))\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, BEST_CHECKPOINT_NAME)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Define model, loss function, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "model = MyLSTM(len(word_to_ix), EMBEDDING_DIM, LSTM_OUT_DIM, BIDIRECTIONAL, DROPOUT_PROB)\n",
    "if CUDA_AVAILABLE:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Start main loop. If checkpoint exists, start from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> no checkpoint found at 'pytorchckpts/3smallnet/checkpoint.pth.tar'. Starting from scratch\n",
      "Epoch: [1][1/83]\tTime 11.982 (11.982)\tData 1.759 (1.759)\tLoss 2.1749 (2.1749)\t\n",
      "Test: [1/15]\tTime 2.565 (2.565)\tLoss 2.1008 (2.1008)\t\n",
      "Test: [2/15]\tTime 2.555 (2.560)\tLoss 2.0802 (2.0905)\t\n",
      "Test: [3/15]\tTime 2.961 (2.693)\tLoss 2.0988 (2.0933)\t\n",
      "Test: [4/15]\tTime 2.966 (2.762)\tLoss 2.1320 (2.1030)\t\n",
      "Test: [5/15]\tTime 2.664 (2.742)\tLoss 2.0771 (2.0978)\t\n",
      "Test: [6/15]\tTime 2.851 (2.760)\tLoss 2.0800 (2.0948)\t\n",
      "Test: [7/15]\tTime 2.655 (2.745)\tLoss 2.1077 (2.0967)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.679 (2.737)\tLoss 2.0955 (2.0965)\t\n",
      "Test: [9/15]\tTime 2.563 (2.717)\tLoss 2.0939 (2.0962)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.569 (2.703)\tLoss 2.0927 (2.0959)\t\n",
      "Test: [11/15]\tTime 2.793 (2.711)\tLoss 2.1293 (2.0989)\t\n",
      "Test: [12/15]\tTime 2.441 (2.688)\tLoss 2.0858 (2.0978)\t\n",
      "Test: [13/15]\tTime 2.850 (2.701)\tLoss 2.1310 (2.1004)\t\n",
      "Test: [14/15]\tTime 2.627 (2.695)\tLoss 2.1453 (2.1036)\t\n",
      "Test: [15/15]\tTime 2.028 (2.651)\tLoss 2.1032 (2.1036)\t\n",
      " * Loss 2.104\n",
      "Epoch: [1][2/83]\tTime 51.049 (31.515)\tData 41.423 (21.591)\tLoss 2.1863 (2.1806)\t\n",
      "Epoch: [1][3/83]\tTime 11.446 (24.826)\tData 1.618 (14.933)\tLoss 2.0171 (2.1261)\t\n",
      "Epoch: [1][4/83]\tTime 11.701 (21.545)\tData 1.607 (11.602)\tLoss 2.0107 (2.0972)\t\n",
      "Epoch: [1][5/83]\tTime 11.530 (19.542)\tData 1.538 (9.589)\tLoss 2.0363 (2.0850)\t\n",
      "Epoch: [1][6/83]\tTime 11.109 (18.136)\tData 1.615 (8.260)\tLoss 1.8535 (2.0465)\t\n",
      "Epoch: [1][7/83]\tTime 11.539 (17.194)\tData 1.599 (7.309)\tLoss 1.8540 (2.0190)\t\n",
      "Epoch: [1][8/83]\tTime 11.344 (16.463)\tData 1.744 (6.613)\tLoss 1.9107 (2.0054)\t\n",
      "Epoch: [1][9/83]\tTime 11.055 (15.862)\tData 1.412 (6.035)\tLoss 1.8583 (1.9891)\t\n",
      "Epoch: [1][10/83]\tTime 11.280 (15.404)\tData 1.725 (5.604)\tLoss 1.7052 (1.9607)\t\n",
      "Epoch: [1][11/83]\tTime 11.747 (15.071)\tData 1.764 (5.255)\tLoss 1.6641 (1.9337)\t\n",
      "Test: [1/15]\tTime 2.595 (2.595)\tLoss 1.8393 (1.8393)\t\n",
      "Test: [2/15]\tTime 2.667 (2.631)\tLoss 1.7107 (1.7750)\t\n",
      "Test: [3/15]\tTime 2.849 (2.704)\tLoss 1.7315 (1.7605)\t\n",
      "Test: [4/15]\tTime 2.982 (2.773)\tLoss 1.8083 (1.7725)\t\n",
      "Test: [5/15]\tTime 2.655 (2.750)\tLoss 1.7383 (1.7656)\t\n",
      "Test: [6/15]\tTime 2.927 (2.779)\tLoss 1.7204 (1.7581)\t\n",
      "Test: [7/15]\tTime 2.585 (2.751)\tLoss 1.7542 (1.7575)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.680 (2.743)\tLoss 1.7580 (1.7576)\t\n",
      "Test: [9/15]\tTime 2.593 (2.726)\tLoss 1.8141 (1.7639)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.570 (2.710)\tLoss 1.6397 (1.7514)\t\n",
      "Test: [11/15]\tTime 2.775 (2.716)\tLoss 1.7498 (1.7513)\t\n",
      "Test: [12/15]\tTime 2.459 (2.695)\tLoss 1.6450 (1.7424)\t\n",
      "Test: [13/15]\tTime 2.848 (2.707)\tLoss 1.7971 (1.7466)\t\n",
      "Test: [14/15]\tTime 2.651 (2.703)\tLoss 1.9077 (1.7581)\t\n",
      "Test: [15/15]\tTime 2.061 (2.660)\tLoss 1.7593 (1.7582)\t\n",
      " * Loss 1.758\n",
      "Epoch: [1][12/83]\tTime 51.151 (18.078)\tData 41.633 (8.287)\tLoss 1.5533 (1.9020)\t\n",
      "Epoch: [1][13/83]\tTime 11.484 (17.571)\tData 1.670 (7.778)\tLoss 2.0001 (1.9096)\t\n",
      "Epoch: [1][14/83]\tTime 11.162 (17.113)\tData 1.637 (7.339)\tLoss 2.0298 (1.9182)\t\n",
      "Epoch: [1][15/83]\tTime 11.318 (16.727)\tData 1.537 (6.952)\tLoss 1.8680 (1.9148)\t\n",
      "Epoch: [1][16/83]\tTime 11.197 (16.381)\tData 1.693 (6.623)\tLoss 1.7195 (1.9026)\t\n",
      "Epoch: [1][17/83]\tTime 11.802 (16.112)\tData 1.541 (6.324)\tLoss 1.6154 (1.8857)\t\n",
      "Epoch: [1][18/83]\tTime 10.996 (15.827)\tData 1.546 (6.059)\tLoss 1.9165 (1.8874)\t\n",
      "Epoch: [1][19/83]\tTime 11.788 (15.615)\tData 1.609 (5.825)\tLoss 1.7188 (1.8785)\t\n",
      "Epoch: [1][20/83]\tTime 11.033 (15.386)\tData 1.533 (5.610)\tLoss 1.7732 (1.8733)\t\n",
      "Epoch: [1][21/83]\tTime 11.946 (15.222)\tData 1.744 (5.426)\tLoss 1.7416 (1.8670)\t\n",
      "Test: [1/15]\tTime 2.560 (2.560)\tLoss 1.6959 (1.6959)\t\n",
      "Test: [2/15]\tTime 2.633 (2.596)\tLoss 1.6723 (1.6841)\t\n",
      "Test: [3/15]\tTime 2.935 (2.709)\tLoss 1.5847 (1.6510)\t\n",
      "Test: [4/15]\tTime 2.978 (2.776)\tLoss 1.6706 (1.6559)\t\n",
      "Test: [5/15]\tTime 2.672 (2.755)\tLoss 1.5280 (1.6303)\t\n",
      "Test: [6/15]\tTime 2.924 (2.784)\tLoss 1.5740 (1.6209)\t\n",
      "Test: [7/15]\tTime 2.596 (2.757)\tLoss 1.6776 (1.6290)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.666 (2.745)\tLoss 1.6479 (1.6314)\t\n",
      "Test: [9/15]\tTime 2.575 (2.727)\tLoss 1.7263 (1.6419)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.657 (2.720)\tLoss 1.4820 (1.6259)\t\n",
      "Test: [11/15]\tTime 2.725 (2.720)\tLoss 1.5849 (1.6222)\t\n",
      "Test: [12/15]\tTime 2.473 (2.699)\tLoss 1.4086 (1.6044)\t\n",
      "Test: [13/15]\tTime 2.856 (2.711)\tLoss 1.6261 (1.6061)\t\n",
      "Test: [14/15]\tTime 2.641 (2.706)\tLoss 1.7689 (1.6177)\t\n",
      "Test: [15/15]\tTime 2.068 (2.664)\tLoss 1.5849 (1.6160)\t\n",
      " * Loss 1.616\n",
      "Epoch: [1][22/83]\tTime 50.773 (16.838)\tData 41.569 (7.069)\tLoss 1.4853 (1.8497)\t\n",
      "Epoch: [1][23/83]\tTime 11.816 (16.620)\tData 1.612 (6.832)\tLoss 1.5604 (1.8371)\t\n",
      "Epoch: [1][24/83]\tTime 11.188 (16.393)\tData 1.655 (6.616)\tLoss 1.7792 (1.8347)\t\n",
      "Epoch: [1][25/83]\tTime 11.737 (16.207)\tData 1.585 (6.415)\tLoss 1.5192 (1.8220)\t\n",
      "Epoch: [1][26/83]\tTime 11.177 (16.013)\tData 1.539 (6.227)\tLoss 1.7295 (1.8185)\t\n",
      "Epoch: [1][27/83]\tTime 11.550 (15.848)\tData 1.725 (6.060)\tLoss 1.4300 (1.8041)\t\n",
      "Epoch: [1][28/83]\tTime 11.098 (15.679)\tData 1.612 (5.902)\tLoss 1.4837 (1.7927)\t\n",
      "Epoch: [1][29/83]\tTime 11.715 (15.542)\tData 1.552 (5.752)\tLoss 1.8992 (1.7963)\t\n",
      "Epoch: [1][30/83]\tTime 11.417 (15.404)\tData 1.884 (5.623)\tLoss 1.5302 (1.7875)\t\n",
      "Epoch: [1][31/83]\tTime 11.981 (15.294)\tData 1.585 (5.492)\tLoss 1.4831 (1.7776)\t\n",
      "Test: [1/15]\tTime 2.590 (2.590)\tLoss 1.5592 (1.5592)\t\n",
      "Test: [2/15]\tTime 2.631 (2.610)\tLoss 1.6556 (1.6074)\t\n",
      "Test: [3/15]\tTime 2.898 (2.706)\tLoss 1.6458 (1.6202)\t\n",
      "Test: [4/15]\tTime 2.981 (2.775)\tLoss 1.7241 (1.6462)\t\n",
      "Test: [5/15]\tTime 2.678 (2.756)\tLoss 1.4537 (1.6077)\t\n",
      "Test: [6/15]\tTime 2.941 (2.786)\tLoss 1.4658 (1.5840)\t\n",
      "Test: [7/15]\tTime 2.603 (2.760)\tLoss 1.5715 (1.5822)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.675 (2.750)\tLoss 1.4062 (1.5602)\t\n",
      "Test: [9/15]\tTime 2.567 (2.729)\tLoss 1.5793 (1.5624)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.648 (2.721)\tLoss 1.3662 (1.5427)\t\n",
      "Test: [11/15]\tTime 2.715 (2.721)\tLoss 1.5968 (1.5477)\t\n",
      "Test: [12/15]\tTime 2.481 (2.701)\tLoss 1.3250 (1.5291)\t\n",
      "Test: [13/15]\tTime 2.844 (2.712)\tLoss 1.5143 (1.5280)\t\n",
      "Test: [14/15]\tTime 2.637 (2.706)\tLoss 1.6705 (1.5381)\t\n",
      "Test: [15/15]\tTime 2.054 (2.663)\tLoss 1.4380 (1.5329)\t\n",
      " * Loss 1.533\n",
      "Epoch: [1][32/83]\tTime 51.135 (16.414)\tData 41.590 (6.621)\tLoss 1.5539 (1.7706)\t\n",
      "Epoch: [1][33/83]\tTime 11.885 (16.277)\tData 1.662 (6.470)\tLoss 1.6118 (1.7658)\t\n",
      "Epoch: [1][34/83]\tTime 11.370 (16.132)\tData 1.831 (6.334)\tLoss 1.4984 (1.7580)\t\n",
      "Epoch: [1][35/83]\tTime 11.872 (16.011)\tData 1.709 (6.202)\tLoss 1.8303 (1.7600)\t\n",
      "Epoch: [1][36/83]\tTime 11.827 (15.894)\tData 1.680 (6.076)\tLoss 1.5439 (1.7540)\t\n",
      "Epoch: [1][37/83]\tTime 11.458 (15.775)\tData 1.653 (5.957)\tLoss 1.4999 (1.7472)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Epoch: [1][38/83]\tTime 11.043 (15.650)\tData 1.674 (5.844)\tLoss 1.6347 (1.7442)\t\n",
      "Epoch: [1][39/83]\tTime 11.572 (15.545)\tData 1.716 (5.738)\tLoss 2.0020 (1.7508)\t\n",
      "Epoch: [1][40/83]\tTime 11.196 (15.437)\tData 1.646 (5.636)\tLoss 1.3605 (1.7411)\t\n",
      "Epoch: [1][41/83]\tTime 11.319 (15.336)\tData 1.502 (5.535)\tLoss 1.6233 (1.7382)\t\n",
      "Test: [1/15]\tTime 2.584 (2.584)\tLoss 1.4333 (1.4333)\t\n",
      "Test: [2/15]\tTime 2.658 (2.621)\tLoss 1.6877 (1.5605)\t\n",
      "Test: [3/15]\tTime 2.938 (2.727)\tLoss 1.5084 (1.5431)\t\n",
      "Test: [4/15]\tTime 3.013 (2.798)\tLoss 1.4856 (1.5288)\t\n",
      "Test: [5/15]\tTime 2.673 (2.773)\tLoss 1.3840 (1.4998)\t\n",
      "Test: [6/15]\tTime 2.930 (2.799)\tLoss 1.3879 (1.4812)\t\n",
      "Test: [7/15]\tTime 2.611 (2.772)\tLoss 1.4922 (1.4827)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.673 (2.760)\tLoss 1.4636 (1.4803)\t\n",
      "Test: [9/15]\tTime 2.576 (2.739)\tLoss 1.5691 (1.4902)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.648 (2.730)\tLoss 1.4121 (1.4824)\t\n",
      "Test: [11/15]\tTime 2.728 (2.730)\tLoss 1.4663 (1.4809)\t\n",
      "Test: [12/15]\tTime 2.482 (2.709)\tLoss 1.2655 (1.4630)\t\n",
      "Test: [13/15]\tTime 2.847 (2.720)\tLoss 1.4938 (1.4653)\t\n",
      "Test: [14/15]\tTime 2.636 (2.714)\tLoss 1.6749 (1.4803)\t\n",
      "Test: [15/15]\tTime 2.044 (2.669)\tLoss 1.3759 (1.4748)\t\n",
      " * Loss 1.475\n",
      "Epoch: [1][42/83]\tTime 51.614 (16.200)\tData 41.674 (6.395)\tLoss 1.3466 (1.7289)\t\n",
      "Epoch: [1][43/83]\tTime 11.684 (16.095)\tData 1.670 (6.285)\tLoss 1.5040 (1.7236)\t\n",
      "Epoch: [1][44/83]\tTime 11.206 (15.984)\tData 1.675 (6.181)\tLoss 1.3553 (1.7153)\t\n",
      "Epoch: [1][45/83]\tTime 11.506 (15.884)\tData 1.677 (6.081)\tLoss 1.6288 (1.7133)\t\n",
      "Epoch: [1][46/83]\tTime 11.045 (15.779)\tData 1.547 (5.982)\tLoss 1.2073 (1.7023)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Epoch: [1][47/83]\tTime 11.385 (15.686)\tData 1.569 (5.888)\tLoss 1.6246 (1.7007)\t\n",
      "Epoch: [1][48/83]\tTime 11.232 (15.593)\tData 1.691 (5.801)\tLoss 1.2236 (1.6907)\t\n",
      "Epoch: [1][49/83]\tTime 11.594 (15.511)\tData 1.713 (5.717)\tLoss 1.1909 (1.6805)\t\n",
      "Epoch: [1][50/83]\tTime 11.359 (15.428)\tData 1.874 (5.640)\tLoss 1.1869 (1.6707)\t\n",
      "Epoch: [1][51/83]\tTime 11.872 (15.359)\tData 1.723 (5.564)\tLoss 1.4371 (1.6661)\t\n",
      "Test: [1/15]\tTime 2.590 (2.590)\tLoss 1.3301 (1.3301)\t\n",
      "Test: [2/15]\tTime 2.636 (2.613)\tLoss 1.4940 (1.4120)\t\n",
      "Test: [3/15]\tTime 2.897 (2.708)\tLoss 1.4039 (1.4093)\t\n",
      "Test: [4/15]\tTime 3.008 (2.783)\tLoss 1.4496 (1.4194)\t\n",
      "Test: [5/15]\tTime 2.695 (2.765)\tLoss 1.3742 (1.4104)\t\n",
      "Test: [6/15]\tTime 2.954 (2.796)\tLoss 1.3014 (1.3922)\t\n",
      "Test: [7/15]\tTime 2.612 (2.770)\tLoss 1.4940 (1.4067)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.673 (2.758)\tLoss 1.3381 (1.3982)\t\n",
      "Test: [9/15]\tTime 2.579 (2.738)\tLoss 1.5387 (1.4138)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.633 (2.728)\tLoss 1.3076 (1.4032)\t\n",
      "Test: [11/15]\tTime 2.718 (2.727)\tLoss 1.4202 (1.4047)\t\n",
      "Test: [12/15]\tTime 2.479 (2.706)\tLoss 1.1955 (1.3873)\t\n",
      "Test: [13/15]\tTime 2.837 (2.716)\tLoss 1.3578 (1.3850)\t\n",
      "Test: [14/15]\tTime 2.625 (2.710)\tLoss 1.6136 (1.4013)\t\n",
      "Test: [15/15]\tTime 2.033 (2.665)\tLoss 1.3633 (1.3993)\t\n",
      " * Loss 1.399\n",
      "Epoch: [1][52/83]\tTime 51.227 (16.048)\tData 41.663 (6.258)\tLoss 1.2518 (1.6581)\t\n",
      "Epoch: [1][53/83]\tTime 12.082 (15.973)\tData 1.686 (6.172)\tLoss 1.6509 (1.6580)\t\n",
      "Epoch: [1][54/83]\tTime 11.088 (15.883)\tData 1.586 (6.087)\tLoss 1.4763 (1.6546)\t\n",
      "Epoch: [1][55/83]\tTime 11.689 (15.807)\tData 1.479 (6.003)\tLoss 1.5735 (1.6531)\t\n",
      "Epoch: [1][56/83]\tTime 10.918 (15.719)\tData 1.506 (5.923)\tLoss 1.3060 (1.6470)\t\n",
      "Epoch: [1][57/83]\tTime 12.065 (15.655)\tData 1.834 (5.851)\tLoss 0.9471 (1.6347)\t\n",
      "Epoch: [1][58/83]\tTime 11.310 (15.580)\tData 1.757 (5.780)\tLoss 1.4169 (1.6309)\t\n",
      "Epoch: [1][59/83]\tTime 11.357 (15.509)\tData 1.583 (5.709)\tLoss 1.1522 (1.6228)\t\n",
      "Epoch: [1][60/83]\tTime 11.242 (15.438)\tData 1.712 (5.642)\tLoss 1.3944 (1.6190)\t\n",
      "Epoch: [1][61/83]\tTime 11.533 (15.374)\tData 1.735 (5.578)\tLoss 1.1829 (1.6118)\t\n",
      "Test: [1/15]\tTime 2.604 (2.604)\tLoss 1.2940 (1.2940)\t\n",
      "Test: [2/15]\tTime 2.644 (2.624)\tLoss 1.5159 (1.4050)\t\n",
      "Test: [3/15]\tTime 2.924 (2.724)\tLoss 1.4176 (1.4092)\t\n",
      "Test: [4/15]\tTime 2.990 (2.790)\tLoss 1.4933 (1.4302)\t\n",
      "Test: [5/15]\tTime 2.667 (2.766)\tLoss 1.4127 (1.4267)\t\n",
      "Test: [6/15]\tTime 2.937 (2.794)\tLoss 1.2887 (1.4037)\t\n",
      "Test: [7/15]\tTime 2.575 (2.763)\tLoss 1.4912 (1.4162)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.663 (2.750)\tLoss 1.3407 (1.4068)\t\n",
      "Test: [9/15]\tTime 2.567 (2.730)\tLoss 1.4693 (1.4137)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.631 (2.720)\tLoss 1.2356 (1.3959)\t\n",
      "Test: [11/15]\tTime 2.728 (2.721)\tLoss 1.3513 (1.3918)\t\n",
      "Test: [12/15]\tTime 2.457 (2.699)\tLoss 1.1998 (1.3758)\t\n",
      "Test: [13/15]\tTime 2.817 (2.708)\tLoss 1.4044 (1.3780)\t\n",
      "Test: [14/15]\tTime 2.698 (2.707)\tLoss 1.8176 (1.4094)\t\n",
      "Test: [15/15]\tTime 1.971 (2.658)\tLoss 1.2791 (1.4026)\t\n",
      " * Loss 1.403\n",
      "FOUND NULL SENTENCE!!!\n",
      "Epoch: [1][62/83]\tTime 50.841 (15.946)\tData 41.420 (6.157)\tLoss 1.4226 (1.6088)\t\n",
      "Epoch: [1][63/83]\tTime 11.450 (15.874)\tData 1.621 (6.085)\tLoss 1.6519 (1.6095)\t\n",
      "Epoch: [1][64/83]\tTime 11.127 (15.800)\tData 1.609 (6.015)\tLoss 1.2057 (1.6032)\t\n",
      "Epoch: [1][65/83]\tTime 11.964 (15.741)\tData 1.824 (5.950)\tLoss 1.6980 (1.6046)\t\n",
      "Epoch: [1][66/83]\tTime 10.878 (15.668)\tData 1.476 (5.882)\tLoss 1.6537 (1.6054)\t\n",
      "Epoch: [1][67/83]\tTime 11.923 (15.612)\tData 1.725 (5.820)\tLoss 1.4249 (1.6027)\t\n",
      "Epoch: [1][68/83]\tTime 12.012 (15.559)\tData 1.828 (5.762)\tLoss 1.5987 (1.6026)\t\n",
      "Epoch: [1][69/83]\tTime 11.582 (15.501)\tData 1.620 (5.702)\tLoss 1.2787 (1.5979)\t\n",
      "Epoch: [1][70/83]\tTime 11.097 (15.438)\tData 1.639 (5.644)\tLoss 1.4986 (1.5965)\t\n",
      "Epoch: [1][71/83]\tTime 11.610 (15.384)\tData 1.695 (5.588)\tLoss 1.2439 (1.5915)\t\n",
      "Test: [1/15]\tTime 2.550 (2.550)\tLoss 1.2796 (1.2796)\t\n",
      "Test: [2/15]\tTime 2.622 (2.586)\tLoss 1.5431 (1.4113)\t\n",
      "Test: [3/15]\tTime 2.896 (2.690)\tLoss 1.3589 (1.3939)\t\n",
      "Test: [4/15]\tTime 2.978 (2.762)\tLoss 1.3912 (1.3932)\t\n",
      "Test: [5/15]\tTime 2.668 (2.743)\tLoss 1.4559 (1.4057)\t\n",
      "Test: [6/15]\tTime 2.950 (2.777)\tLoss 1.2742 (1.3838)\t\n",
      "Test: [7/15]\tTime 2.594 (2.751)\tLoss 1.3881 (1.3844)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.674 (2.742)\tLoss 1.2891 (1.3725)\t\n",
      "Test: [9/15]\tTime 2.584 (2.724)\tLoss 1.3979 (1.3753)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.621 (2.714)\tLoss 1.2400 (1.3618)\t\n",
      "Test: [11/15]\tTime 2.689 (2.711)\tLoss 1.3354 (1.3594)\t\n",
      "Test: [12/15]\tTime 2.437 (2.689)\tLoss 1.1428 (1.3413)\t\n",
      "Test: [13/15]\tTime 2.810 (2.698)\tLoss 1.2929 (1.3376)\t\n",
      "Test: [14/15]\tTime 2.679 (2.697)\tLoss 1.5964 (1.3561)\t\n",
      "Test: [15/15]\tTime 1.958 (2.647)\tLoss 1.2125 (1.3485)\t\n",
      " * Loss 1.349\n",
      "Epoch: [1][72/83]\tTime 51.384 (15.884)\tData 41.437 (6.086)\tLoss 1.2095 (1.5862)\t\n",
      "Epoch: [1][73/83]\tTime 11.814 (15.828)\tData 1.813 (6.027)\tLoss 1.5660 (1.5860)\t\n",
      "Epoch: [1][74/83]\tTime 11.205 (15.766)\tData 1.750 (5.969)\tLoss 1.4034 (1.5835)\t\n",
      "Epoch: [1][75/83]\tTime 11.693 (15.712)\tData 1.696 (5.912)\tLoss 1.4298 (1.5814)\t\n",
      "Epoch: [1][76/83]\tTime 10.902 (15.648)\tData 1.526 (5.855)\tLoss 1.7392 (1.5835)\t\n",
      "Epoch: [1][77/83]\tTime 11.352 (15.593)\tData 1.546 (5.799)\tLoss 1.2887 (1.5797)\t\n",
      "Epoch: [1][78/83]\tTime 11.036 (15.534)\tData 1.592 (5.745)\tLoss 1.4649 (1.5782)\t\n",
      "Epoch: [1][79/83]\tTime 11.555 (15.484)\tData 1.751 (5.694)\tLoss 1.3578 (1.5754)\t\n",
      "Epoch: [1][80/83]\tTime 10.977 (15.427)\tData 1.498 (5.642)\tLoss 1.5758 (1.5754)\t\n",
      "Epoch: [1][81/83]\tTime 11.388 (15.378)\tData 1.662 (5.593)\tLoss 1.5267 (1.5748)\t\n",
      "Test: [1/15]\tTime 2.553 (2.553)\tLoss 1.2873 (1.2873)\t\n",
      "Test: [2/15]\tTime 2.608 (2.581)\tLoss 1.5376 (1.4125)\t\n",
      "Test: [3/15]\tTime 2.911 (2.691)\tLoss 1.4077 (1.4109)\t\n",
      "Test: [4/15]\tTime 2.932 (2.751)\tLoss 1.3884 (1.4053)\t\n",
      "Test: [5/15]\tTime 2.648 (2.730)\tLoss 1.4397 (1.4121)\t\n",
      "Test: [6/15]\tTime 2.910 (2.760)\tLoss 1.3668 (1.4046)\t\n",
      "Test: [7/15]\tTime 2.664 (2.747)\tLoss 1.4446 (1.4103)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.607 (2.729)\tLoss 1.3409 (1.4016)\t\n",
      "Test: [9/15]\tTime 2.552 (2.709)\tLoss 1.3713 (1.3983)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.625 (2.701)\tLoss 1.2860 (1.3870)\t\n",
      "Test: [11/15]\tTime 2.705 (2.701)\tLoss 1.3378 (1.3826)\t\n",
      "Test: [12/15]\tTime 2.449 (2.680)\tLoss 1.0955 (1.3586)\t\n",
      "Test: [13/15]\tTime 2.830 (2.692)\tLoss 1.3976 (1.3616)\t\n",
      "Test: [14/15]\tTime 2.710 (2.693)\tLoss 1.5470 (1.3749)\t\n",
      "Test: [15/15]\tTime 1.967 (2.645)\tLoss 1.2471 (1.3682)\t\n",
      " * Loss 1.368\n",
      "Epoch: [1][82/83]\tTime 51.178 (15.814)\tData 41.218 (6.027)\tLoss 1.5601 (1.5747)\t\n",
      "Epoch: [1][83/83]\tTime 12.074 (15.769)\tData 1.666 (5.975)\tLoss 1.8168 (1.5776)\t\n",
      "Test: [1/15]\tTime 2.544 (2.544)\tLoss 1.3032 (1.3032)\t\n",
      "Test: [2/15]\tTime 2.631 (2.587)\tLoss 1.4899 (1.3965)\t\n",
      "Test: [3/15]\tTime 2.892 (2.689)\tLoss 1.3842 (1.3924)\t\n",
      "Test: [4/15]\tTime 2.956 (2.756)\tLoss 1.4385 (1.4039)\t\n",
      "Test: [5/15]\tTime 2.635 (2.731)\tLoss 1.3957 (1.4023)\t\n",
      "Test: [6/15]\tTime 2.864 (2.754)\tLoss 1.3477 (1.3932)\t\n",
      "Test: [7/15]\tTime 2.640 (2.737)\tLoss 1.4311 (1.3986)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.657 (2.727)\tLoss 1.3070 (1.3872)\t\n",
      "Test: [9/15]\tTime 2.544 (2.707)\tLoss 1.3915 (1.3876)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.597 (2.696)\tLoss 1.3219 (1.3811)\t\n",
      "Test: [11/15]\tTime 2.765 (2.702)\tLoss 1.3614 (1.3793)\t\n",
      "Test: [12/15]\tTime 2.442 (2.681)\tLoss 1.0810 (1.3544)\t\n",
      "Test: [13/15]\tTime 2.811 (2.691)\tLoss 1.3834 (1.3567)\t\n",
      "Test: [14/15]\tTime 2.640 (2.687)\tLoss 1.5206 (1.3684)\t\n",
      "Test: [15/15]\tTime 2.028 (2.643)\tLoss 1.2216 (1.3606)\t\n",
      " * Loss 1.361\n",
      "1.3606414633585995 better than previous best loss of inf\n",
      "Epoch: [2][1/83]\tTime 11.424 (11.424)\tData 1.542 (1.542)\tLoss 1.0385 (1.0385)\t\n",
      "Test: [1/15]\tTime 2.551 (2.551)\tLoss 1.2774 (1.2774)\t\n",
      "Test: [2/15]\tTime 2.540 (2.545)\tLoss 1.4933 (1.3854)\t\n",
      "Test: [3/15]\tTime 2.976 (2.689)\tLoss 1.4156 (1.3955)\t\n",
      "Test: [4/15]\tTime 2.977 (2.761)\tLoss 1.4054 (1.3979)\t\n",
      "Test: [5/15]\tTime 2.628 (2.734)\tLoss 1.3706 (1.3925)\t\n",
      "Test: [6/15]\tTime 2.837 (2.751)\tLoss 1.3469 (1.3849)\t\n",
      "Test: [7/15]\tTime 2.669 (2.740)\tLoss 1.4513 (1.3944)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.663 (2.730)\tLoss 1.3371 (1.3872)\t\n",
      "Test: [9/15]\tTime 2.571 (2.713)\tLoss 1.3593 (1.3841)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.555 (2.697)\tLoss 1.3058 (1.3763)\t\n",
      "Test: [11/15]\tTime 2.760 (2.702)\tLoss 1.3305 (1.3721)\t\n",
      "Test: [12/15]\tTime 2.452 (2.682)\tLoss 1.0695 (1.3469)\t\n",
      "Test: [13/15]\tTime 2.805 (2.691)\tLoss 1.3867 (1.3500)\t\n",
      "Test: [14/15]\tTime 2.611 (2.685)\tLoss 1.4900 (1.3600)\t\n",
      "Test: [15/15]\tTime 2.028 (2.641)\tLoss 1.2018 (1.3516)\t\n",
      " * Loss 1.352\n",
      "Epoch: [2][2/83]\tTime 51.434 (31.429)\tData 41.255 (21.398)\tLoss 0.9531 (0.9958)\t\n",
      "Epoch: [2][3/83]\tTime 11.484 (24.781)\tData 1.507 (14.768)\tLoss 1.0512 (1.0143)\t\n",
      "Epoch: [2][4/83]\tTime 11.829 (21.543)\tData 1.726 (11.507)\tLoss 1.0836 (1.0316)\t\n",
      "Epoch: [2][5/83]\tTime 11.513 (19.537)\tData 1.622 (9.530)\tLoss 0.9266 (1.0106)\t\n",
      "Epoch: [2][6/83]\tTime 11.712 (18.233)\tData 1.785 (8.239)\tLoss 1.1873 (1.0401)\t\n",
      "Epoch: [2][7/83]\tTime 11.628 (17.289)\tData 1.621 (7.294)\tLoss 1.4414 (1.0974)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Epoch: [2][8/83]\tTime 11.165 (16.524)\tData 1.771 (6.604)\tLoss 1.0538 (1.0920)\t\n",
      "Epoch: [2][9/83]\tTime 11.573 (15.974)\tData 1.654 (6.054)\tLoss 1.2136 (1.1055)\t\n",
      "Epoch: [2][10/83]\tTime 11.704 (15.547)\tData 1.579 (5.606)\tLoss 1.3293 (1.1279)\t\n",
      "Epoch: [2][11/83]\tTime 11.704 (15.197)\tData 1.714 (5.252)\tLoss 1.4493 (1.1571)\t\n",
      "Test: [1/15]\tTime 2.579 (2.579)\tLoss 1.2245 (1.2245)\t\n",
      "Test: [2/15]\tTime 2.659 (2.619)\tLoss 1.5408 (1.3827)\t\n",
      "Test: [3/15]\tTime 2.863 (2.700)\tLoss 1.3730 (1.3795)\t\n",
      "Test: [4/15]\tTime 2.976 (2.769)\tLoss 1.4013 (1.3849)\t\n",
      "Test: [5/15]\tTime 2.631 (2.741)\tLoss 1.4239 (1.3927)\t\n",
      "Test: [6/15]\tTime 2.899 (2.768)\tLoss 1.3138 (1.3795)\t\n",
      "Test: [7/15]\tTime 2.576 (2.740)\tLoss 1.3016 (1.3684)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.658 (2.730)\tLoss 1.2529 (1.3540)\t\n",
      "Test: [9/15]\tTime 2.548 (2.710)\tLoss 1.4327 (1.3627)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.556 (2.694)\tLoss 1.1886 (1.3453)\t\n",
      "Test: [11/15]\tTime 2.759 (2.700)\tLoss 1.3083 (1.3419)\t\n",
      "Test: [12/15]\tTime 2.456 (2.680)\tLoss 1.0577 (1.3183)\t\n",
      "Test: [13/15]\tTime 2.821 (2.691)\tLoss 1.3330 (1.3194)\t\n",
      "Test: [14/15]\tTime 2.627 (2.686)\tLoss 1.4999 (1.3323)\t\n",
      "Test: [15/15]\tTime 2.023 (2.642)\tLoss 1.2249 (1.3266)\t\n",
      " * Loss 1.327\n",
      "Epoch: [2][12/83]\tTime 50.940 (18.176)\tData 41.420 (8.266)\tLoss 1.2403 (1.1640)\t\n",
      "Epoch: [2][13/83]\tTime 11.457 (17.659)\tData 1.524 (7.748)\tLoss 1.4490 (1.1859)\t\n",
      "Epoch: [2][14/83]\tTime 11.342 (17.208)\tData 1.751 (7.319)\tLoss 0.8589 (1.1626)\t\n",
      "Epoch: [2][15/83]\tTime 12.073 (16.866)\tData 1.710 (6.945)\tLoss 0.9131 (1.1459)\t\n",
      "Epoch: [2][16/83]\tTime 10.986 (16.498)\tData 1.522 (6.606)\tLoss 1.5753 (1.1728)\t\n",
      "Epoch: [2][17/83]\tTime 11.539 (16.206)\tData 1.461 (6.304)\tLoss 1.3584 (1.1837)\t\n",
      "Epoch: [2][18/83]\tTime 11.271 (15.932)\tData 1.796 (6.053)\tLoss 1.2985 (1.1901)\t\n",
      "Epoch: [2][19/83]\tTime 11.501 (15.699)\tData 1.683 (5.823)\tLoss 1.1905 (1.1901)\t\n",
      "Epoch: [2][20/83]\tTime 11.083 (15.468)\tData 1.612 (5.613)\tLoss 1.1603 (1.1886)\t\n",
      "Epoch: [2][21/83]\tTime 11.707 (15.289)\tData 1.588 (5.421)\tLoss 1.0294 (1.1810)\t\n",
      "Test: [1/15]\tTime 2.557 (2.557)\tLoss 1.2690 (1.2690)\t\n",
      "Test: [2/15]\tTime 2.637 (2.597)\tLoss 1.5902 (1.4296)\t\n",
      "Test: [3/15]\tTime 2.856 (2.683)\tLoss 1.3743 (1.4112)\t\n",
      "Test: [4/15]\tTime 2.941 (2.748)\tLoss 1.3903 (1.4060)\t\n",
      "Test: [5/15]\tTime 2.611 (2.720)\tLoss 1.4094 (1.4066)\t\n",
      "Test: [6/15]\tTime 2.894 (2.749)\tLoss 1.2903 (1.3873)\t\n",
      "Test: [7/15]\tTime 2.558 (2.722)\tLoss 1.3072 (1.3758)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.644 (2.712)\tLoss 1.2363 (1.3584)\t\n",
      "Test: [9/15]\tTime 2.546 (2.694)\tLoss 1.4188 (1.3651)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.602 (2.685)\tLoss 1.2652 (1.3551)\t\n",
      "Test: [11/15]\tTime 2.688 (2.685)\tLoss 1.3473 (1.3544)\t\n",
      "Test: [12/15]\tTime 2.448 (2.665)\tLoss 1.1037 (1.3335)\t\n",
      "Test: [13/15]\tTime 2.792 (2.675)\tLoss 1.3855 (1.3375)\t\n",
      "Test: [14/15]\tTime 2.605 (2.670)\tLoss 1.5348 (1.3516)\t\n",
      "Test: [15/15]\tTime 2.019 (2.627)\tLoss 1.2205 (1.3447)\t\n",
      " * Loss 1.345\n",
      "Epoch: [2][22/83]\tTime 50.444 (16.887)\tData 40.954 (7.036)\tLoss 1.4539 (1.1934)\t\n",
      "Epoch: [2][23/83]\tTime 11.298 (16.644)\tData 1.528 (6.797)\tLoss 1.2195 (1.1946)\t\n",
      "Epoch: [2][24/83]\tTime 10.941 (16.406)\tData 1.545 (6.578)\tLoss 1.0640 (1.1891)\t\n",
      "Epoch: [2][25/83]\tTime 11.234 (16.199)\tData 1.473 (6.374)\tLoss 1.1820 (1.1888)\t\n",
      "Epoch: [2][26/83]\tTime 11.000 (16.000)\tData 1.501 (6.186)\tLoss 0.9369 (1.1791)\t\n",
      "Epoch: [2][27/83]\tTime 12.006 (15.852)\tData 1.588 (6.016)\tLoss 1.0413 (1.1740)\t\n",
      "Epoch: [2][28/83]\tTime 11.858 (15.709)\tData 1.664 (5.861)\tLoss 1.0282 (1.1688)\t\n",
      "Epoch: [2][29/83]\tTime 11.267 (15.556)\tData 1.524 (5.711)\tLoss 1.3015 (1.1734)\t\n",
      "Epoch: [2][30/83]\tTime 11.826 (15.431)\tData 1.701 (5.577)\tLoss 0.9484 (1.1659)\t\n",
      "Epoch: [2][31/83]\tTime 11.574 (15.307)\tData 1.573 (5.448)\tLoss 1.3309 (1.1712)\t\n",
      "Test: [1/15]\tTime 2.543 (2.543)\tLoss 1.2060 (1.2060)\t\n",
      "Test: [2/15]\tTime 2.612 (2.578)\tLoss 1.5497 (1.3778)\t\n",
      "Test: [3/15]\tTime 2.860 (2.672)\tLoss 1.3521 (1.3693)\t\n",
      "Test: [4/15]\tTime 2.951 (2.742)\tLoss 1.3383 (1.3615)\t\n",
      "Test: [5/15]\tTime 2.657 (2.725)\tLoss 1.3624 (1.3617)\t\n",
      "Test: [6/15]\tTime 2.941 (2.761)\tLoss 1.2101 (1.3364)\t\n",
      "Test: [7/15]\tTime 2.609 (2.739)\tLoss 1.2222 (1.3201)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.668 (2.730)\tLoss 1.1734 (1.3018)\t\n",
      "Test: [9/15]\tTime 2.582 (2.714)\tLoss 1.3393 (1.3059)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.630 (2.705)\tLoss 1.1986 (1.2952)\t\n",
      "Test: [11/15]\tTime 2.699 (2.705)\tLoss 1.2518 (1.2913)\t\n",
      "Test: [12/15]\tTime 2.470 (2.685)\tLoss 1.0637 (1.2723)\t\n",
      "Test: [13/15]\tTime 2.788 (2.693)\tLoss 1.3583 (1.2789)\t\n",
      "Test: [14/15]\tTime 2.604 (2.687)\tLoss 1.5030 (1.2949)\t\n",
      "Test: [15/15]\tTime 2.028 (2.643)\tLoss 1.2111 (1.2905)\t\n",
      " * Loss 1.291\n",
      "Epoch: [2][32/83]\tTime 50.905 (16.419)\tData 41.432 (6.573)\tLoss 0.8978 (1.1627)\t\n",
      "Epoch: [2][33/83]\tTime 11.485 (16.270)\tData 1.546 (6.420)\tLoss 1.1991 (1.1638)\t\n",
      "Epoch: [2][34/83]\tTime 11.507 (16.130)\tData 1.569 (6.278)\tLoss 0.9210 (1.1566)\t\n",
      "Epoch: [2][35/83]\tTime 11.444 (15.996)\tData 1.566 (6.143)\tLoss 1.0430 (1.1534)\t\n",
      "Epoch: [2][36/83]\tTime 11.150 (15.861)\tData 1.638 (6.018)\tLoss 0.9152 (1.1468)\t\n",
      "Epoch: [2][37/83]\tTime 11.483 (15.743)\tData 1.529 (5.897)\tLoss 1.0826 (1.1450)\t\n",
      "Epoch: [2][38/83]\tTime 12.004 (15.645)\tData 1.846 (5.790)\tLoss 1.0036 (1.1413)\t\n",
      "Epoch: [2][39/83]\tTime 11.503 (15.538)\tData 1.550 (5.681)\tLoss 1.1747 (1.1422)\t\n",
      "Epoch: [2][40/83]\tTime 10.895 (15.422)\tData 1.461 (5.576)\tLoss 0.8452 (1.1348)\t\n",
      "Epoch: [2][41/83]\tTime 11.445 (15.325)\tData 1.636 (5.480)\tLoss 1.2177 (1.1368)\t\n",
      "Test: [1/15]\tTime 2.538 (2.538)\tLoss 1.2055 (1.2055)\t\n",
      "Test: [2/15]\tTime 2.614 (2.576)\tLoss 1.5005 (1.3530)\t\n",
      "Test: [3/15]\tTime 2.887 (2.680)\tLoss 1.4811 (1.3957)\t\n",
      "Test: [4/15]\tTime 2.955 (2.749)\tLoss 1.3952 (1.3956)\t\n",
      "Test: [5/15]\tTime 2.660 (2.731)\tLoss 1.3296 (1.3824)\t\n",
      "Test: [6/15]\tTime 2.893 (2.758)\tLoss 1.2064 (1.3531)\t\n",
      "Test: [7/15]\tTime 2.567 (2.731)\tLoss 1.2223 (1.3344)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.695 (2.726)\tLoss 1.1428 (1.3104)\t\n",
      "Test: [9/15]\tTime 2.601 (2.712)\tLoss 1.3319 (1.3128)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.615 (2.703)\tLoss 1.1590 (1.2974)\t\n",
      "Test: [11/15]\tTime 2.720 (2.704)\tLoss 1.3240 (1.2999)\t\n",
      "Test: [12/15]\tTime 2.444 (2.682)\tLoss 1.0056 (1.2753)\t\n",
      "Test: [13/15]\tTime 2.847 (2.695)\tLoss 1.2925 (1.2766)\t\n",
      "Test: [14/15]\tTime 2.684 (2.694)\tLoss 1.4663 (1.2902)\t\n",
      "Test: [15/15]\tTime 2.026 (2.650)\tLoss 1.1843 (1.2846)\t\n",
      " * Loss 1.285\n",
      "Epoch: [2][42/83]\tTime 51.668 (16.191)\tData 41.406 (6.335)\tLoss 1.2192 (1.1387)\t\n",
      "Epoch: [2][43/83]\tTime 11.731 (16.087)\tData 1.786 (6.229)\tLoss 0.7277 (1.1292)\t\n",
      "Epoch: [2][44/83]\tTime 10.864 (15.968)\tData 1.416 (6.120)\tLoss 1.1947 (1.1307)\t\n",
      "Epoch: [2][45/83]\tTime 11.499 (15.869)\tData 1.477 (6.017)\tLoss 1.5652 (1.1403)\t\n",
      "Epoch: [2][46/83]\tTime 11.290 (15.769)\tData 1.687 (5.923)\tLoss 0.9672 (1.1366)\t\n",
      "Epoch: [2][47/83]\tTime 12.128 (15.692)\tData 1.750 (5.834)\tLoss 1.0576 (1.1349)\t\n",
      "Epoch: [2][48/83]\tTime 11.937 (15.614)\tData 1.929 (5.752)\tLoss 1.0483 (1.1331)\t\n",
      "Epoch: [2][49/83]\tTime 11.696 (15.534)\tData 1.708 (5.670)\tLoss 1.1145 (1.1327)\t\n",
      "Epoch: [2][50/83]\tTime 11.155 (15.446)\tData 1.483 (5.586)\tLoss 1.1483 (1.1330)\t\n",
      "Epoch: [2][51/83]\tTime 11.353 (15.366)\tData 1.605 (5.508)\tLoss 1.0272 (1.1309)\t\n",
      "Test: [1/15]\tTime 2.593 (2.593)\tLoss 1.1415 (1.1415)\t\n",
      "Test: [2/15]\tTime 2.602 (2.597)\tLoss 1.4871 (1.3143)\t\n",
      "Test: [3/15]\tTime 2.865 (2.687)\tLoss 1.4640 (1.3642)\t\n",
      "Test: [4/15]\tTime 2.981 (2.760)\tLoss 1.3740 (1.3667)\t\n",
      "Test: [5/15]\tTime 2.641 (2.737)\tLoss 1.3533 (1.3640)\t\n",
      "Test: [6/15]\tTime 2.898 (2.763)\tLoss 1.2371 (1.3428)\t\n",
      "Test: [7/15]\tTime 2.609 (2.741)\tLoss 1.2054 (1.3232)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.646 (2.730)\tLoss 1.0914 (1.2942)\t\n",
      "Test: [9/15]\tTime 2.571 (2.712)\tLoss 1.3078 (1.2957)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.836 (2.724)\tLoss 1.0714 (1.2733)\t\n",
      "Test: [11/15]\tTime 2.706 (2.723)\tLoss 1.3735 (1.2824)\t\n",
      "Test: [12/15]\tTime 2.739 (2.724)\tLoss 0.9971 (1.2586)\t\n",
      "Test: [13/15]\tTime 2.835 (2.733)\tLoss 1.2972 (1.2616)\t\n",
      "Test: [14/15]\tTime 2.611 (2.724)\tLoss 1.4513 (1.2752)\t\n",
      "Test: [15/15]\tTime 2.015 (2.677)\tLoss 1.1913 (1.2707)\t\n",
      " * Loss 1.271\n",
      "Epoch: [2][52/83]\tTime 51.157 (16.054)\tData 41.721 (6.205)\tLoss 0.7803 (1.1242)\t\n",
      "Epoch: [2][53/83]\tTime 11.918 (15.976)\tData 1.666 (6.119)\tLoss 1.3406 (1.1283)\t\n",
      "Epoch: [2][54/83]\tTime 11.002 (15.884)\tData 1.582 (6.035)\tLoss 1.4517 (1.1343)\t\n",
      "Epoch: [2][55/83]\tTime 11.264 (15.800)\tData 1.577 (5.954)\tLoss 1.0107 (1.1320)\t\n",
      "Epoch: [2][56/83]\tTime 11.344 (15.720)\tData 1.733 (5.878)\tLoss 1.2580 (1.1343)\t\n",
      "Epoch: [2][57/83]\tTime 11.587 (15.648)\tData 1.557 (5.803)\tLoss 1.1295 (1.1342)\t\n",
      "Epoch: [2][58/83]\tTime 11.003 (15.568)\tData 1.546 (5.729)\tLoss 1.0738 (1.1332)\t\n",
      "Epoch: [2][59/83]\tTime 11.562 (15.500)\tData 1.715 (5.661)\tLoss 1.2361 (1.1349)\t\n",
      "Epoch: [2][60/83]\tTime 11.258 (15.429)\tData 1.763 (5.596)\tLoss 1.1404 (1.1350)\t\n",
      "Epoch: [2][61/83]\tTime 11.827 (15.370)\tData 1.617 (5.531)\tLoss 1.2695 (1.1372)\t\n",
      "Test: [1/15]\tTime 2.527 (2.527)\tLoss 1.0877 (1.0877)\t\n",
      "Test: [2/15]\tTime 2.596 (2.561)\tLoss 1.4817 (1.2847)\t\n",
      "Test: [3/15]\tTime 2.866 (2.663)\tLoss 1.4558 (1.3418)\t\n",
      "Test: [4/15]\tTime 2.939 (2.732)\tLoss 1.3429 (1.3421)\t\n",
      "Test: [5/15]\tTime 2.626 (2.711)\tLoss 1.2917 (1.3320)\t\n",
      "Test: [6/15]\tTime 2.892 (2.741)\tLoss 1.2028 (1.3105)\t\n",
      "Test: [7/15]\tTime 2.569 (2.716)\tLoss 1.2774 (1.3057)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.649 (2.708)\tLoss 1.1628 (1.2879)\t\n",
      "Test: [9/15]\tTime 2.546 (2.690)\tLoss 1.2659 (1.2854)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.603 (2.681)\tLoss 1.0840 (1.2653)\t\n",
      "Test: [11/15]\tTime 2.676 (2.681)\tLoss 1.3157 (1.2699)\t\n",
      "Test: [12/15]\tTime 2.436 (2.660)\tLoss 1.0136 (1.2485)\t\n",
      "Test: [13/15]\tTime 2.791 (2.670)\tLoss 1.2849 (1.2513)\t\n",
      "Test: [14/15]\tTime 2.611 (2.666)\tLoss 1.4767 (1.2674)\t\n",
      "Test: [15/15]\tTime 2.016 (2.623)\tLoss 1.2361 (1.2658)\t\n",
      " * Loss 1.266\n",
      "Epoch: [2][62/83]\tTime 50.733 (15.941)\tData 41.134 (6.105)\tLoss 1.2223 (1.1386)\t\n",
      "Epoch: [2][63/83]\tTime 11.860 (15.876)\tData 1.690 (6.035)\tLoss 1.5038 (1.1444)\t\n",
      "Epoch: [2][64/83]\tTime 11.857 (15.813)\tData 1.701 (5.967)\tLoss 1.2754 (1.1464)\t\n",
      "Epoch: [2][65/83]\tTime 11.449 (15.746)\tData 1.731 (5.902)\tLoss 1.2377 (1.1478)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Epoch: [2][66/83]\tTime 11.199 (15.677)\tData 1.737 (5.839)\tLoss 1.3581 (1.1510)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Epoch: [2][67/83]\tTime 11.829 (15.620)\tData 1.587 (5.776)\tLoss 0.7460 (1.1450)\t\n",
      "Epoch: [2][68/83]\tTime 11.020 (15.552)\tData 1.494 (5.713)\tLoss 1.7988 (1.1546)\t\n",
      "Epoch: [2][69/83]\tTime 11.889 (15.499)\tData 1.675 (5.654)\tLoss 0.9145 (1.1511)\t\n",
      "Epoch: [2][70/83]\tTime 11.729 (15.445)\tData 1.846 (5.600)\tLoss 1.0700 (1.1499)\t\n",
      "Epoch: [2][71/83]\tTime 11.516 (15.390)\tData 1.627 (5.544)\tLoss 0.7778 (1.1447)\t\n",
      "Test: [1/15]\tTime 2.529 (2.529)\tLoss 1.1012 (1.1012)\t\n",
      "Test: [2/15]\tTime 2.596 (2.563)\tLoss 1.4097 (1.2554)\t\n",
      "Test: [3/15]\tTime 2.885 (2.670)\tLoss 1.3907 (1.3005)\t\n",
      "Test: [4/15]\tTime 2.958 (2.742)\tLoss 1.3708 (1.3181)\t\n",
      "Test: [5/15]\tTime 2.634 (2.720)\tLoss 1.3293 (1.3203)\t\n",
      "Test: [6/15]\tTime 2.892 (2.749)\tLoss 1.2161 (1.3030)\t\n",
      "Test: [7/15]\tTime 2.548 (2.720)\tLoss 1.2188 (1.2910)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.651 (2.712)\tLoss 1.1752 (1.2765)\t\n",
      "Test: [9/15]\tTime 2.537 (2.692)\tLoss 1.2433 (1.2728)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.595 (2.682)\tLoss 1.1019 (1.2557)\t\n",
      "Test: [11/15]\tTime 2.688 (2.683)\tLoss 1.2404 (1.2543)\t\n",
      "Test: [12/15]\tTime 2.455 (2.664)\tLoss 1.0052 (1.2336)\t\n",
      "Test: [13/15]\tTime 2.805 (2.675)\tLoss 1.2584 (1.2355)\t\n",
      "Test: [14/15]\tTime 2.621 (2.671)\tLoss 1.4897 (1.2536)\t\n",
      "Test: [15/15]\tTime 2.043 (2.629)\tLoss 1.1300 (1.2471)\t\n",
      " * Loss 1.247\n",
      "Epoch: [2][72/83]\tTime 50.720 (15.880)\tData 41.119 (6.038)\tLoss 1.1672 (1.1450)\t\n",
      "Epoch: [2][73/83]\tTime 11.386 (15.819)\tData 1.563 (5.977)\tLoss 1.3682 (1.1481)\t\n",
      "Epoch: [2][74/83]\tTime 10.967 (15.753)\tData 1.493 (5.916)\tLoss 0.8885 (1.1446)\t\n",
      "Epoch: [2][75/83]\tTime 11.585 (15.698)\tData 1.682 (5.860)\tLoss 1.7376 (1.1525)\t\n",
      "Epoch: [2][76/83]\tTime 11.175 (15.638)\tData 1.746 (5.805)\tLoss 1.0789 (1.1515)\t\n",
      "Epoch: [2][77/83]\tTime 11.386 (15.583)\tData 1.634 (5.751)\tLoss 1.1194 (1.1511)\t\n",
      "Epoch: [2][78/83]\tTime 10.842 (15.522)\tData 1.506 (5.697)\tLoss 1.4198 (1.1545)\t\n",
      "Epoch: [2][79/83]\tTime 11.802 (15.475)\tData 1.834 (5.648)\tLoss 1.0279 (1.1529)\t\n",
      "Epoch: [2][80/83]\tTime 11.506 (15.425)\tData 1.632 (5.598)\tLoss 0.7362 (1.1477)\t\n",
      "Epoch: [2][81/83]\tTime 11.856 (15.381)\tData 1.694 (5.550)\tLoss 1.1435 (1.1477)\t\n",
      "Test: [1/15]\tTime 2.542 (2.542)\tLoss 1.1760 (1.1760)\t\n",
      "Test: [2/15]\tTime 2.644 (2.593)\tLoss 1.3680 (1.2720)\t\n",
      "Test: [3/15]\tTime 2.852 (2.680)\tLoss 1.3445 (1.2962)\t\n",
      "Test: [4/15]\tTime 2.934 (2.743)\tLoss 1.3119 (1.3001)\t\n",
      "Test: [5/15]\tTime 2.618 (2.718)\tLoss 1.3063 (1.3013)\t\n",
      "Test: [6/15]\tTime 2.884 (2.746)\tLoss 1.3297 (1.3061)\t\n",
      "Test: [7/15]\tTime 2.586 (2.723)\tLoss 1.2881 (1.3035)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.663 (2.715)\tLoss 1.3391 (1.3080)\t\n",
      "Test: [9/15]\tTime 2.559 (2.698)\tLoss 1.2129 (1.2974)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.603 (2.688)\tLoss 1.1806 (1.2857)\t\n",
      "Test: [11/15]\tTime 2.677 (2.687)\tLoss 1.2315 (1.2808)\t\n",
      "Test: [12/15]\tTime 2.423 (2.665)\tLoss 1.0340 (1.2602)\t\n",
      "Test: [13/15]\tTime 2.797 (2.676)\tLoss 1.2374 (1.2585)\t\n",
      "Test: [14/15]\tTime 2.683 (2.676)\tLoss 1.4335 (1.2710)\t\n",
      "Test: [15/15]\tTime 1.957 (2.628)\tLoss 1.1599 (1.2651)\t\n",
      " * Loss 1.265\n",
      "Epoch: [2][82/83]\tTime 50.582 (15.811)\tData 40.963 (5.981)\tLoss 1.2561 (1.1490)\t\n",
      "Epoch: [2][83/83]\tTime 11.765 (15.762)\tData 1.657 (5.929)\tLoss 1.2610 (1.1503)\t\n",
      "Test: [1/15]\tTime 2.556 (2.556)\tLoss 1.1662 (1.1662)\t\n",
      "Test: [2/15]\tTime 2.558 (2.557)\tLoss 1.3815 (1.2738)\t\n",
      "Test: [3/15]\tTime 2.896 (2.670)\tLoss 1.3213 (1.2897)\t\n",
      "Test: [4/15]\tTime 2.992 (2.750)\tLoss 1.2914 (1.2901)\t\n",
      "Test: [5/15]\tTime 2.604 (2.721)\tLoss 1.3569 (1.3035)\t\n",
      "Test: [6/15]\tTime 3.018 (2.770)\tLoss 1.2837 (1.3002)\t\n",
      "Test: [7/15]\tTime 2.628 (2.750)\tLoss 1.2379 (1.2913)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.619 (2.734)\tLoss 1.2800 (1.2899)\t\n",
      "Test: [9/15]\tTime 2.493 (2.707)\tLoss 1.2413 (1.2845)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.602 (2.696)\tLoss 1.1709 (1.2731)\t\n",
      "Test: [11/15]\tTime 2.744 (2.701)\tLoss 1.2846 (1.2742)\t\n",
      "Test: [12/15]\tTime 2.455 (2.680)\tLoss 1.0235 (1.2533)\t\n",
      "Test: [13/15]\tTime 2.720 (2.683)\tLoss 1.2742 (1.2549)\t\n",
      "Test: [14/15]\tTime 2.665 (2.682)\tLoss 1.4563 (1.2693)\t\n",
      "Test: [15/15]\tTime 2.002 (2.637)\tLoss 1.1524 (1.2631)\t\n",
      " * Loss 1.263\n",
      "1.2631121721482814 better than previous best loss of 1.3606414633585995\n",
      "Epoch: [3][1/83]\tTime 11.845 (11.845)\tData 1.841 (1.841)\tLoss 0.9752 (0.9752)\t\n",
      "Test: [1/15]\tTime 2.493 (2.493)\tLoss 1.2054 (1.2054)\t\n",
      "Test: [2/15]\tTime 2.598 (2.546)\tLoss 1.3765 (1.2910)\t\n",
      "Test: [3/15]\tTime 2.969 (2.687)\tLoss 1.3905 (1.3242)\t\n",
      "Test: [4/15]\tTime 2.980 (2.760)\tLoss 1.3050 (1.3194)\t\n",
      "Test: [5/15]\tTime 2.570 (2.722)\tLoss 1.3562 (1.3267)\t\n",
      "Test: [6/15]\tTime 2.937 (2.758)\tLoss 1.2957 (1.3216)\t\n",
      "Test: [7/15]\tTime 2.657 (2.744)\tLoss 1.2316 (1.3087)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.644 (2.731)\tLoss 1.2552 (1.3020)\t\n",
      "Test: [9/15]\tTime 2.504 (2.706)\tLoss 1.2417 (1.2953)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.605 (2.696)\tLoss 1.1489 (1.2807)\t\n",
      "Test: [11/15]\tTime 2.744 (2.700)\tLoss 1.2094 (1.2742)\t\n",
      "Test: [12/15]\tTime 2.440 (2.678)\tLoss 1.0423 (1.2549)\t\n",
      "Test: [13/15]\tTime 2.746 (2.684)\tLoss 1.2622 (1.2554)\t\n",
      "Test: [14/15]\tTime 2.665 (2.682)\tLoss 1.4624 (1.2702)\t\n",
      "Test: [15/15]\tTime 2.026 (2.639)\tLoss 1.1204 (1.2623)\t\n",
      " * Loss 1.262\n",
      "Epoch: [3][2/83]\tTime 51.378 (31.611)\tData 41.218 (21.529)\tLoss 1.2806 (1.1279)\t\n",
      "Epoch: [3][3/83]\tTime 11.520 (24.914)\tData 1.569 (14.876)\tLoss 0.8805 (1.0454)\t\n",
      "Epoch: [3][4/83]\tTime 11.024 (21.442)\tData 1.544 (11.543)\tLoss 1.3332 (1.1174)\t\n",
      "Epoch: [3][5/83]\tTime 11.721 (19.497)\tData 1.760 (9.586)\tLoss 0.8780 (1.0695)\t\n",
      "Epoch: [3][6/83]\tTime 11.595 (18.180)\tData 1.666 (8.266)\tLoss 1.1674 (1.0858)\t\n",
      "Epoch: [3][7/83]\tTime 11.762 (17.264)\tData 1.611 (7.316)\tLoss 0.9230 (1.0626)\t\n",
      "Epoch: [3][8/83]\tTime 11.994 (16.605)\tData 1.825 (6.629)\tLoss 0.8212 (1.0324)\t\n",
      "Epoch: [3][9/83]\tTime 11.293 (16.015)\tData 1.702 (6.082)\tLoss 1.0471 (1.0340)\t\n",
      "Epoch: [3][10/83]\tTime 11.352 (15.548)\tData 1.454 (5.619)\tLoss 1.0482 (1.0354)\t\n",
      "Epoch: [3][11/83]\tTime 11.147 (15.148)\tData 1.603 (5.254)\tLoss 1.1241 (1.0435)\t\n",
      "Test: [1/15]\tTime 2.531 (2.531)\tLoss 1.1498 (1.1498)\t\n",
      "Test: [2/15]\tTime 2.522 (2.527)\tLoss 1.3632 (1.2565)\t\n",
      "Test: [3/15]\tTime 2.910 (2.654)\tLoss 1.4076 (1.3069)\t\n",
      "Test: [4/15]\tTime 2.988 (2.738)\tLoss 1.3479 (1.3171)\t\n",
      "Test: [5/15]\tTime 2.637 (2.718)\tLoss 1.2928 (1.3123)\t\n",
      "Test: [6/15]\tTime 2.819 (2.735)\tLoss 1.2698 (1.3052)\t\n",
      "Test: [7/15]\tTime 2.622 (2.719)\tLoss 1.2566 (1.2982)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.603 (2.704)\tLoss 1.1843 (1.2840)\t\n",
      "Test: [9/15]\tTime 2.460 (2.677)\tLoss 1.2707 (1.2825)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.584 (2.668)\tLoss 1.1777 (1.2720)\t\n",
      "Test: [11/15]\tTime 2.760 (2.676)\tLoss 1.2515 (1.2702)\t\n",
      "Test: [12/15]\tTime 2.437 (2.656)\tLoss 0.9560 (1.2440)\t\n",
      "Test: [13/15]\tTime 2.723 (2.661)\tLoss 1.2902 (1.2475)\t\n",
      "Test: [14/15]\tTime 2.664 (2.662)\tLoss 1.4311 (1.2607)\t\n",
      "Test: [15/15]\tTime 2.022 (2.619)\tLoss 1.0652 (1.2504)\t\n",
      " * Loss 1.250\n",
      "Epoch: [3][12/83]\tTime 50.824 (18.121)\tData 41.020 (8.234)\tLoss 1.0649 (1.0453)\t\n",
      "Epoch: [3][13/83]\tTime 11.194 (17.588)\tData 1.641 (7.727)\tLoss 0.7976 (1.0262)\t\n",
      "Epoch: [3][14/83]\tTime 11.562 (17.158)\tData 1.743 (7.300)\tLoss 1.1656 (1.0362)\t\n",
      "Epoch: [3][15/83]\tTime 11.352 (16.771)\tData 1.737 (6.929)\tLoss 0.9293 (1.0291)\t\n",
      "Epoch: [3][16/83]\tTime 11.483 (16.440)\tData 1.551 (6.593)\tLoss 0.8366 (1.0170)\t\n",
      "Epoch: [3][17/83]\tTime 10.857 (16.112)\tData 1.420 (6.289)\tLoss 0.6770 (0.9970)\t\n",
      "Epoch: [3][18/83]\tTime 11.478 (15.854)\tData 1.618 (6.029)\tLoss 0.7461 (0.9831)\t\n",
      "Epoch: [3][19/83]\tTime 11.681 (15.635)\tData 1.752 (5.804)\tLoss 0.8540 (0.9763)\t\n",
      "Epoch: [3][20/83]\tTime 11.716 (15.439)\tData 1.625 (5.595)\tLoss 0.7398 (0.9645)\t\n",
      "Epoch: [3][21/83]\tTime 11.259 (15.240)\tData 1.749 (5.412)\tLoss 1.1053 (0.9712)\t\n",
      "Test: [1/15]\tTime 2.542 (2.542)\tLoss 1.0842 (1.0842)\t\n",
      "Test: [2/15]\tTime 2.572 (2.557)\tLoss 1.3386 (1.2114)\t\n",
      "Test: [3/15]\tTime 2.894 (2.669)\tLoss 1.3658 (1.2629)\t\n",
      "Test: [4/15]\tTime 2.935 (2.736)\tLoss 1.3944 (1.2958)\t\n",
      "Test: [5/15]\tTime 2.623 (2.713)\tLoss 1.4077 (1.3181)\t\n",
      "Test: [6/15]\tTime 2.819 (2.731)\tLoss 1.2287 (1.3032)\t\n",
      "Test: [7/15]\tTime 2.621 (2.715)\tLoss 1.2956 (1.3021)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.661 (2.708)\tLoss 1.1209 (1.2795)\t\n",
      "Test: [9/15]\tTime 2.557 (2.692)\tLoss 1.2719 (1.2786)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.531 (2.675)\tLoss 1.0840 (1.2592)\t\n",
      "Test: [11/15]\tTime 2.735 (2.681)\tLoss 1.1536 (1.2496)\t\n",
      "Test: [12/15]\tTime 2.453 (2.662)\tLoss 0.9769 (1.2269)\t\n",
      "Test: [13/15]\tTime 2.710 (2.666)\tLoss 1.2191 (1.2263)\t\n",
      "Test: [14/15]\tTime 2.653 (2.665)\tLoss 1.4336 (1.2411)\t\n",
      "Test: [15/15]\tTime 2.011 (2.621)\tLoss 1.0898 (1.2331)\t\n",
      " * Loss 1.233\n",
      "Epoch: [3][22/83]\tTime 51.251 (16.877)\tData 41.035 (7.031)\tLoss 0.7588 (0.9615)\t\n",
      "Epoch: [3][23/83]\tTime 11.025 (16.622)\tData 1.579 (6.794)\tLoss 0.8262 (0.9556)\t\n",
      "Epoch: [3][24/83]\tTime 11.782 (16.421)\tData 1.509 (6.574)\tLoss 0.7720 (0.9480)\t\n",
      "Epoch: [3][25/83]\tTime 11.788 (16.235)\tData 1.639 (6.377)\tLoss 0.8163 (0.9427)\t\n",
      "Epoch: [3][26/83]\tTime 11.574 (16.056)\tData 1.684 (6.196)\tLoss 0.9168 (0.9417)\t\n",
      "Epoch: [3][27/83]\tTime 11.245 (15.878)\tData 1.705 (6.030)\tLoss 0.7525 (0.9347)\t\n",
      "Epoch: [3][28/83]\tTime 11.434 (15.719)\tData 1.645 (5.873)\tLoss 1.0407 (0.9385)\t\n",
      "Epoch: [3][29/83]\tTime 11.112 (15.560)\tData 1.609 (5.726)\tLoss 1.0599 (0.9427)\t\n",
      "Epoch: [3][30/83]\tTime 11.709 (15.432)\tData 1.599 (5.588)\tLoss 0.8933 (0.9410)\t\n",
      "Epoch: [3][31/83]\tTime 11.735 (15.313)\tData 1.568 (5.459)\tLoss 1.0279 (0.9438)\t\n",
      "Test: [1/15]\tTime 2.534 (2.534)\tLoss 1.1765 (1.1765)\t\n",
      "Test: [2/15]\tTime 2.555 (2.544)\tLoss 1.4166 (1.2966)\t\n",
      "Test: [3/15]\tTime 2.900 (2.663)\tLoss 1.4438 (1.3456)\t\n",
      "Test: [4/15]\tTime 2.935 (2.731)\tLoss 1.3543 (1.3478)\t\n",
      "Test: [5/15]\tTime 2.627 (2.710)\tLoss 1.4509 (1.3684)\t\n",
      "Test: [6/15]\tTime 2.832 (2.730)\tLoss 1.3134 (1.3592)\t\n",
      "Test: [7/15]\tTime 2.624 (2.715)\tLoss 1.2295 (1.3407)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 3.676 (2.835)\tLoss 1.2177 (1.3253)\t\n",
      "Test: [9/15]\tTime 3.412 (2.899)\tLoss 1.3384 (1.3268)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.533 (2.863)\tLoss 1.1690 (1.3110)\t\n",
      "Test: [11/15]\tTime 2.758 (2.853)\tLoss 1.2131 (1.3021)\t\n",
      "Test: [12/15]\tTime 2.443 (2.819)\tLoss 1.0462 (1.2808)\t\n",
      "Test: [13/15]\tTime 2.819 (2.819)\tLoss 1.2436 (1.2779)\t\n",
      "Test: [14/15]\tTime 2.601 (2.803)\tLoss 1.4662 (1.2914)\t\n",
      "Test: [15/15]\tTime 2.019 (2.751)\tLoss 1.1147 (1.2821)\t\n",
      " * Loss 1.282\n",
      "Epoch: [3][32/83]\tTime 52.924 (16.488)\tData 43.083 (6.635)\tLoss 1.4601 (0.9600)\t\n",
      "Epoch: [3][33/83]\tTime 11.009 (16.322)\tData 1.491 (6.479)\tLoss 1.7514 (0.9840)\t\n",
      "Epoch: [3][34/83]\tTime 11.390 (16.177)\tData 1.644 (6.336)\tLoss 0.8392 (0.9797)\t\n",
      "Epoch: [3][35/83]\tTime 10.938 (16.027)\tData 1.489 (6.198)\tLoss 1.0920 (0.9829)\t\n",
      "Epoch: [3][36/83]\tTime 11.887 (15.912)\tData 1.665 (6.072)\tLoss 0.9226 (0.9812)\t\n",
      "Epoch: [3][37/83]\tTime 11.188 (15.785)\tData 1.634 (5.952)\tLoss 1.2246 (0.9878)\t\n",
      "Epoch: [3][38/83]\tTime 11.547 (15.673)\tData 1.584 (5.837)\tLoss 0.8159 (0.9833)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Epoch: [3][39/83]\tTime 10.924 (15.551)\tData 1.514 (5.726)\tLoss 0.9076 (0.9813)\t\n",
      "Epoch: [3][40/83]\tTime 11.585 (15.452)\tData 1.640 (5.624)\tLoss 1.0638 (0.9834)\t\n",
      "Epoch: [3][41/83]\tTime 11.054 (15.345)\tData 1.616 (5.526)\tLoss 0.8477 (0.9801)\t\n",
      "Test: [1/15]\tTime 2.535 (2.535)\tLoss 1.2006 (1.2006)\t\n",
      "Test: [2/15]\tTime 2.619 (2.577)\tLoss 1.4961 (1.3484)\t\n",
      "Test: [3/15]\tTime 2.812 (2.655)\tLoss 1.5539 (1.4169)\t\n",
      "Test: [4/15]\tTime 2.902 (2.717)\tLoss 1.4288 (1.4199)\t\n",
      "Test: [5/15]\tTime 2.627 (2.699)\tLoss 1.4512 (1.4261)\t\n",
      "Test: [6/15]\tTime 2.811 (2.718)\tLoss 1.3310 (1.4103)\t\n",
      "Test: [7/15]\tTime 2.610 (2.702)\tLoss 1.2853 (1.3924)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.638 (2.694)\tLoss 1.2200 (1.3709)\t\n",
      "Test: [9/15]\tTime 2.543 (2.677)\tLoss 1.3455 (1.3681)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.529 (2.663)\tLoss 1.1664 (1.3479)\t\n",
      "Test: [11/15]\tTime 2.762 (2.672)\tLoss 1.3625 (1.3492)\t\n",
      "Test: [12/15]\tTime 2.463 (2.654)\tLoss 1.0015 (1.3202)\t\n",
      "Test: [13/15]\tTime 2.816 (2.667)\tLoss 1.2687 (1.3163)\t\n",
      "Test: [14/15]\tTime 2.600 (2.662)\tLoss 1.3974 (1.3221)\t\n",
      "Test: [15/15]\tTime 1.995 (2.617)\tLoss 1.0840 (1.3096)\t\n",
      " * Loss 1.310\n",
      "Epoch: [3][42/83]\tTime 50.519 (16.182)\tData 40.685 (6.363)\tLoss 1.1204 (0.9834)\t\n",
      "Epoch: [3][43/83]\tTime 10.798 (16.057)\tData 1.437 (6.249)\tLoss 0.9180 (0.9819)\t\n",
      "Epoch: [3][44/83]\tTime 11.588 (15.956)\tData 1.573 (6.143)\tLoss 0.8530 (0.9790)\t\n",
      "Epoch: [3][45/83]\tTime 11.439 (15.855)\tData 1.557 (6.041)\tLoss 0.9148 (0.9776)\t\n",
      "Epoch: [3][46/83]\tTime 11.537 (15.761)\tData 1.552 (5.943)\tLoss 1.0314 (0.9787)\t\n",
      "Epoch: [3][47/83]\tTime 11.058 (15.661)\tData 1.653 (5.852)\tLoss 0.6599 (0.9719)\t\n",
      "Epoch: [3][48/83]\tTime 11.441 (15.573)\tData 1.474 (5.761)\tLoss 1.0027 (0.9726)\t\n",
      "Epoch: [3][49/83]\tTime 10.807 (15.476)\tData 1.402 (5.672)\tLoss 1.0342 (0.9738)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Epoch: [3][50/83]\tTime 11.322 (15.393)\tData 1.541 (5.589)\tLoss 1.0574 (0.9755)\t\n",
      "Epoch: [3][51/83]\tTime 11.033 (15.307)\tData 1.637 (5.512)\tLoss 0.5609 (0.9674)\t\n",
      "Test: [1/15]\tTime 2.539 (2.539)\tLoss 1.1448 (1.1448)\t\n",
      "Test: [2/15]\tTime 2.612 (2.575)\tLoss 1.4221 (1.2835)\t\n",
      "Test: [3/15]\tTime 2.828 (2.660)\tLoss 1.4969 (1.3546)\t\n",
      "Test: [4/15]\tTime 2.909 (2.722)\tLoss 1.3868 (1.3627)\t\n",
      "Test: [5/15]\tTime 2.620 (2.702)\tLoss 1.3574 (1.3616)\t\n",
      "Test: [6/15]\tTime 2.902 (2.735)\tLoss 1.1624 (1.3284)\t\n",
      "Test: [7/15]\tTime 2.559 (2.710)\tLoss 1.2075 (1.3111)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.625 (2.699)\tLoss 1.1929 (1.2964)\t\n",
      "Test: [9/15]\tTime 2.565 (2.684)\tLoss 1.1863 (1.2841)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.600 (2.676)\tLoss 1.0889 (1.2646)\t\n",
      "Test: [11/15]\tTime 2.670 (2.675)\tLoss 1.2463 (1.2629)\t\n",
      "Test: [12/15]\tTime 2.410 (2.653)\tLoss 0.9540 (1.2372)\t\n",
      "Test: [13/15]\tTime 2.775 (2.663)\tLoss 1.2556 (1.2386)\t\n",
      "Test: [14/15]\tTime 2.580 (2.657)\tLoss 1.3590 (1.2472)\t\n",
      "Test: [15/15]\tTime 2.013 (2.614)\tLoss 1.0819 (1.2385)\t\n",
      " * Loss 1.239\n",
      "Epoch: [3][52/83]\tTime 51.196 (15.998)\tData 41.030 (6.195)\tLoss 0.5756 (0.9598)\t\n",
      "Epoch: [3][53/83]\tTime 11.646 (15.916)\tData 1.705 (6.110)\tLoss 0.8862 (0.9585)\t\n",
      "Epoch: [3][54/83]\tTime 11.823 (15.840)\tData 1.679 (6.028)\tLoss 1.0283 (0.9598)\t\n",
      "Epoch: [3][55/83]\tTime 11.311 (15.757)\tData 1.810 (5.951)\tLoss 1.0110 (0.9607)\t\n",
      "Epoch: [3][56/83]\tTime 11.708 (15.685)\tData 1.743 (5.876)\tLoss 0.6932 (0.9559)\t\n",
      "Epoch: [3][57/83]\tTime 10.812 (15.600)\tData 1.453 (5.798)\tLoss 0.9796 (0.9563)\t\n",
      "Epoch: [3][58/83]\tTime 11.879 (15.535)\tData 1.721 (5.728)\tLoss 0.7753 (0.9532)\t\n",
      "Epoch: [3][59/83]\tTime 10.987 (15.458)\tData 1.482 (5.656)\tLoss 0.8037 (0.9507)\t\n",
      "Epoch: [3][60/83]\tTime 11.708 (15.396)\tData 1.580 (5.588)\tLoss 0.6960 (0.9464)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Epoch: [3][61/83]\tTime 10.848 (15.321)\tData 1.517 (5.522)\tLoss 1.0549 (0.9482)\t\n",
      "Test: [1/15]\tTime 2.544 (2.544)\tLoss 1.1779 (1.1779)\t\n",
      "Test: [2/15]\tTime 2.603 (2.573)\tLoss 1.3876 (1.2827)\t\n",
      "Test: [3/15]\tTime 2.890 (2.679)\tLoss 1.5594 (1.3749)\t\n",
      "Test: [4/15]\tTime 2.948 (2.746)\tLoss 1.3081 (1.3582)\t\n",
      "Test: [5/15]\tTime 2.638 (2.725)\tLoss 1.4443 (1.3754)\t\n",
      "Test: [6/15]\tTime 2.906 (2.755)\tLoss 1.1363 (1.3356)\t\n",
      "Test: [7/15]\tTime 2.564 (2.728)\tLoss 1.1892 (1.3147)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.652 (2.718)\tLoss 1.2042 (1.3009)\t\n",
      "Test: [9/15]\tTime 2.533 (2.698)\tLoss 1.2949 (1.3002)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.605 (2.688)\tLoss 1.1282 (1.2830)\t\n",
      "Test: [11/15]\tTime 2.666 (2.686)\tLoss 1.2351 (1.2786)\t\n",
      "Test: [12/15]\tTime 2.439 (2.666)\tLoss 0.9939 (1.2549)\t\n",
      "Test: [13/15]\tTime 2.791 (2.675)\tLoss 1.3352 (1.2611)\t\n",
      "Test: [14/15]\tTime 2.601 (2.670)\tLoss 1.4397 (1.2738)\t\n",
      "Test: [15/15]\tTime 2.015 (2.626)\tLoss 1.2090 (1.2704)\t\n",
      " * Loss 1.270\n",
      "Epoch: [3][62/83]\tTime 51.235 (15.901)\tData 41.047 (6.095)\tLoss 1.3458 (0.9546)\t\n",
      "Epoch: [3][63/83]\tTime 11.040 (15.823)\tData 1.546 (6.022)\tLoss 0.9281 (0.9542)\t\n",
      "Epoch: [3][64/83]\tTime 11.850 (15.761)\tData 1.628 (5.954)\tLoss 0.8056 (0.9519)\t\n",
      "Epoch: [3][65/83]\tTime 10.814 (15.685)\tData 1.534 (5.886)\tLoss 0.8545 (0.9504)\t\n",
      "Epoch: [3][66/83]\tTime 11.351 (15.620)\tData 1.555 (5.820)\tLoss 1.0804 (0.9523)\t\n",
      "Epoch: [3][67/83]\tTime 11.336 (15.556)\tData 1.788 (5.760)\tLoss 0.8450 (0.9507)\t\n",
      "Epoch: [3][68/83]\tTime 11.774 (15.500)\tData 1.920 (5.703)\tLoss 0.7801 (0.9482)\t\n",
      "Epoch: [3][69/83]\tTime 11.699 (15.445)\tData 1.753 (5.646)\tLoss 1.0391 (0.9495)\t\n",
      "Epoch: [3][70/83]\tTime 12.119 (15.397)\tData 1.799 (5.591)\tLoss 0.9337 (0.9493)\t\n",
      "Epoch: [3][71/83]\tTime 11.126 (15.337)\tData 1.609 (5.535)\tLoss 1.0293 (0.9504)\t\n",
      "Test: [1/15]\tTime 2.547 (2.547)\tLoss 1.1427 (1.1427)\t\n",
      "Test: [2/15]\tTime 2.619 (2.583)\tLoss 1.3899 (1.2663)\t\n",
      "Test: [3/15]\tTime 2.848 (2.671)\tLoss 1.5062 (1.3463)\t\n",
      "Test: [4/15]\tTime 2.934 (2.737)\tLoss 1.3146 (1.3383)\t\n",
      "Test: [5/15]\tTime 2.631 (2.716)\tLoss 1.5024 (1.3712)\t\n",
      "Test: [6/15]\tTime 2.902 (2.747)\tLoss 1.1533 (1.3348)\t\n",
      "Test: [7/15]\tTime 2.578 (2.723)\tLoss 1.2564 (1.3236)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.642 (2.713)\tLoss 1.2680 (1.3167)\t\n",
      "Test: [9/15]\tTime 2.549 (2.695)\tLoss 1.2757 (1.3121)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.630 (2.688)\tLoss 1.0746 (1.2884)\t\n",
      "Test: [11/15]\tTime 2.685 (2.688)\tLoss 1.3359 (1.2927)\t\n",
      "Test: [12/15]\tTime 2.435 (2.667)\tLoss 0.9568 (1.2647)\t\n",
      "Test: [13/15]\tTime 2.815 (2.678)\tLoss 1.3723 (1.2730)\t\n",
      "Test: [14/15]\tTime 2.689 (2.679)\tLoss 1.3880 (1.2812)\t\n",
      "Test: [15/15]\tTime 1.958 (2.631)\tLoss 1.1687 (1.2753)\t\n",
      " * Loss 1.275\n",
      "Epoch: [3][72/83]\tTime 50.935 (15.832)\tData 41.116 (6.029)\tLoss 0.6785 (0.9467)\t\n",
      "Epoch: [3][73/83]\tTime 10.982 (15.765)\tData 1.595 (5.969)\tLoss 1.0547 (0.9482)\t\n",
      "Epoch: [3][74/83]\tTime 11.816 (15.712)\tData 1.669 (5.910)\tLoss 1.1717 (0.9512)\t\n",
      "Epoch: [3][75/83]\tTime 11.119 (15.651)\tData 1.685 (5.854)\tLoss 1.2902 (0.9557)\t\n",
      "Epoch: [3][76/83]\tTime 11.712 (15.599)\tData 1.898 (5.802)\tLoss 0.6694 (0.9519)\t\n",
      "Epoch: [3][77/83]\tTime 11.537 (15.546)\tData 1.597 (5.747)\tLoss 0.8647 (0.9508)\t\n",
      "Epoch: [3][78/83]\tTime 11.475 (15.494)\tData 1.639 (5.695)\tLoss 1.0456 (0.9520)\t\n",
      "Epoch: [3][79/83]\tTime 11.038 (15.437)\tData 1.619 (5.643)\tLoss 1.0531 (0.9533)\t\n",
      "Epoch: [3][80/83]\tTime 12.043 (15.395)\tData 1.690 (5.594)\tLoss 0.8118 (0.9515)\t\n",
      "Epoch: [3][81/83]\tTime 11.063 (15.342)\tData 1.466 (5.543)\tLoss 1.3374 (0.9563)\t\n",
      "Test: [1/15]\tTime 2.550 (2.550)\tLoss 1.1394 (1.1394)\t\n",
      "Test: [2/15]\tTime 2.611 (2.581)\tLoss 1.2962 (1.2178)\t\n",
      "Test: [3/15]\tTime 2.864 (2.675)\tLoss 1.5497 (1.3284)\t\n",
      "Test: [4/15]\tTime 2.979 (2.751)\tLoss 1.2946 (1.3200)\t\n",
      "Test: [5/15]\tTime 2.633 (2.727)\tLoss 1.4248 (1.3409)\t\n",
      "Test: [6/15]\tTime 2.903 (2.757)\tLoss 1.1248 (1.3049)\t\n",
      "Test: [7/15]\tTime 2.560 (2.729)\tLoss 1.2107 (1.2915)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.635 (2.717)\tLoss 1.2544 (1.2868)\t\n",
      "Test: [9/15]\tTime 2.553 (2.699)\tLoss 1.2546 (1.2832)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.620 (2.691)\tLoss 1.0819 (1.2631)\t\n",
      "Test: [11/15]\tTime 2.698 (2.691)\tLoss 1.2296 (1.2601)\t\n",
      "Test: [12/15]\tTime 2.436 (2.670)\tLoss 0.9584 (1.2349)\t\n",
      "Test: [13/15]\tTime 2.805 (2.680)\tLoss 1.4307 (1.2500)\t\n",
      "Test: [14/15]\tTime 2.663 (2.679)\tLoss 1.4224 (1.2623)\t\n",
      "Test: [15/15]\tTime 1.956 (2.631)\tLoss 1.1037 (1.2540)\t\n",
      " * Loss 1.254\n",
      "Epoch: [3][82/83]\tTime 51.151 (15.778)\tData 41.169 (5.977)\tLoss 0.7388 (0.9536)\t\n",
      "Epoch: [3][83/83]\tTime 11.114 (15.722)\tData 1.546 (5.924)\tLoss 0.9068 (0.9531)\t\n",
      "Test: [1/15]\tTime 2.535 (2.535)\tLoss 1.1670 (1.1670)\t\n",
      "Test: [2/15]\tTime 2.548 (2.542)\tLoss 1.3066 (1.2368)\t\n",
      "Test: [3/15]\tTime 2.933 (2.672)\tLoss 1.5331 (1.3356)\t\n",
      "Test: [4/15]\tTime 2.971 (2.747)\tLoss 1.3124 (1.3298)\t\n",
      "Test: [5/15]\tTime 2.626 (2.723)\tLoss 1.3711 (1.3380)\t\n",
      "Test: [6/15]\tTime 2.829 (2.740)\tLoss 1.1533 (1.3073)\t\n",
      "Test: [7/15]\tTime 2.652 (2.728)\tLoss 1.1838 (1.2896)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [8/15]\tTime 2.633 (2.716)\tLoss 1.2697 (1.2871)\t\n",
      "Test: [9/15]\tTime 2.555 (2.698)\tLoss 1.2253 (1.2803)\t\n",
      "FOUND NULL SENTENCE!!!\n",
      "Test: [10/15]\tTime 2.543 (2.682)\tLoss 1.0948 (1.2617)\t\n",
      "Test: [11/15]\tTime 2.755 (2.689)\tLoss 1.2452 (1.2602)\t\n",
      "Test: [12/15]\tTime 2.437 (2.668)\tLoss 0.9560 (1.2349)\t\n",
      "Test: [13/15]\tTime 2.723 (2.672)\tLoss 1.4051 (1.2480)\t\n",
      "Test: [14/15]\tTime 2.676 (2.673)\tLoss 1.4437 (1.2619)\t\n",
      "Test: [15/15]\tTime 2.012 (2.628)\tLoss 1.0640 (1.2515)\t\n",
      " * Loss 1.252\n",
      "1.2515232921542978 better than previous best loss of 1.2631121721482814\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(CHECKPOINT_NAME):\n",
    "    print(\"=> loading checkpoint '{}'\".format(CHECKPOINT_NAME))\n",
    "    checkpoint = torch.load(CHECKPOINT_NAME)\n",
    "    EPOCH = checkpoint['epoch']\n",
    "    BEST_LOSS = checkpoint['best_loss']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "          .format(CHECKPOINT_NAME, checkpoint['epoch']))\n",
    "else:\n",
    "    print(\"=> no checkpoint found at '{}'. Starting from scratch\".format(CHECKPOINT_NAME))\n",
    "    \n",
    "for epoch in range(EPOCH, NUM_EPOCHS):\n",
    "    train(train_dataset_loader, model, loss_fn, optimizer, epoch + 1, val_dataset_loader)\n",
    "    loss = validate(val_dataset_loader, model, loss_fn)\n",
    "    \n",
    "    if loss < BEST_LOSS:\n",
    "        print('{} better than previous best loss of {}'.format(loss, BEST_LOSS))\n",
    "        BEST_LOSS = loss\n",
    "        is_best = True\n",
    "    else:\n",
    "        is_best = Falsee\n",
    "    \n",
    "    save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_loss': BEST_LOSS,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.577482152702217"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Generate submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint 'pytorchckpts/1increasedseqlength/model_best.pth.tar'\n",
      "=> loaded checkpoint 'pytorchckpts/1increasedseqlength/model_best.pth.tar' (epoch 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MyLSTM (\n",
       "  (embedding): Embedding(4588, 128)\n",
       "  (lstm): LSTM(384, 196, bidirectional=True)\n",
       "  (linear): Linear (392 -> 9)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists(CHECKPOINT_NAME):\n",
    "    print(\"=> loading checkpoint '{}'\".format(BEST_CHECKPOINT_NAME))\n",
    "    checkpoint = torch.load(BEST_CHECKPOINT_NAME)\n",
    "    EPOCH = checkpoint['epoch']\n",
    "    BEST_LOSS = checkpoint['best_loss']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "          .format(BEST_CHECKPOINT_NAME, checkpoint['epoch']))\n",
    "else:\n",
    "    raise Exception(\"=> no checkpoint found at '{}'. Cannot generate submission\".format(BEST_CHECKPOINT_NAME))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "test_dataset = SentencesDataset(df_test, word_to_ix, word_limit=TEXT_WORD_LIMIT)\n",
    "test_dataset_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "probas = []\n",
    "for i, test_batch in enumerate(test_dataset_loader):\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "    log_probas, indices = model.forward(test_batch)\n",
    "    _, orig_indices = torch.sort(indices)\n",
    "    log_probas = log_probas.data.cpu()[orig_indices]\n",
    "    probas.append(log_probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 5.9359e-02  1.0463e-01  4.0240e-02  ...   2.3442e-01  2.4609e-02  1.1846e-02\n",
       " 1.1853e-01  8.7576e-02  4.6069e-03  ...   4.1419e-01  8.8140e-03  4.6486e-03\n",
       " 1.9026e-01  1.6926e-01  2.7235e-02  ...   1.7260e-01  1.2332e-02  8.0794e-03\n",
       "                ...                   ⋱                   ...                \n",
       " 6.2367e-02  9.1292e-02  2.1270e-02  ...   1.5810e-01  1.8779e-02  9.2628e-03\n",
       " 9.6648e-02  1.9787e-01  7.6937e-02  ...   3.8579e-01  2.2144e-02  9.0063e-03\n",
       " 1.2727e-01  1.4228e-01  2.8646e-02  ...   3.9214e-01  1.8107e-02  5.9765e-03\n",
       "[torch.FloatTensor of size 5668x9]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas = torch.exp(torch.cat(probas, dim=0))\n",
    "probas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05935883,  0.1046342 ,  0.04023978,  0.29929909,  0.20350039,\n",
       "        0.02209731,  0.23441558,  0.02460939,  0.01184551], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 8]),\n",
       " array([ 160,  120,    5, 1987,  289,   37, 3069,    1]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.argmax(probas.numpy(), axis=1), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class1</th>\n",
       "      <th>class2</th>\n",
       "      <th>class3</th>\n",
       "      <th>class4</th>\n",
       "      <th>class5</th>\n",
       "      <th>class6</th>\n",
       "      <th>class7</th>\n",
       "      <th>class8</th>\n",
       "      <th>class9</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.059359</td>\n",
       "      <td>0.104634</td>\n",
       "      <td>0.040240</td>\n",
       "      <td>0.299299</td>\n",
       "      <td>0.203500</td>\n",
       "      <td>0.022097</td>\n",
       "      <td>0.234416</td>\n",
       "      <td>0.024609</td>\n",
       "      <td>0.011846</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.118530</td>\n",
       "      <td>0.087576</td>\n",
       "      <td>0.004607</td>\n",
       "      <td>0.160645</td>\n",
       "      <td>0.194606</td>\n",
       "      <td>0.006379</td>\n",
       "      <td>0.414194</td>\n",
       "      <td>0.008814</td>\n",
       "      <td>0.004649</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.190263</td>\n",
       "      <td>0.169263</td>\n",
       "      <td>0.027235</td>\n",
       "      <td>0.249991</td>\n",
       "      <td>0.149442</td>\n",
       "      <td>0.020791</td>\n",
       "      <td>0.172604</td>\n",
       "      <td>0.012332</td>\n",
       "      <td>0.008079</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.114072</td>\n",
       "      <td>0.100352</td>\n",
       "      <td>0.042725</td>\n",
       "      <td>0.349307</td>\n",
       "      <td>0.152393</td>\n",
       "      <td>0.023283</td>\n",
       "      <td>0.172244</td>\n",
       "      <td>0.030713</td>\n",
       "      <td>0.014911</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.071063</td>\n",
       "      <td>0.114701</td>\n",
       "      <td>0.216790</td>\n",
       "      <td>0.182893</td>\n",
       "      <td>0.128843</td>\n",
       "      <td>0.026818</td>\n",
       "      <td>0.224724</td>\n",
       "      <td>0.025380</td>\n",
       "      <td>0.008788</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     class1    class2    class3    class4    class5    class6    class7  \\\n",
       "0  0.059359  0.104634  0.040240  0.299299  0.203500  0.022097  0.234416   \n",
       "1  0.118530  0.087576  0.004607  0.160645  0.194606  0.006379  0.414194   \n",
       "2  0.190263  0.169263  0.027235  0.249991  0.149442  0.020791  0.172604   \n",
       "3  0.114072  0.100352  0.042725  0.349307  0.152393  0.023283  0.172244   \n",
       "4  0.071063  0.114701  0.216790  0.182893  0.128843  0.026818  0.224724   \n",
       "\n",
       "     class8    class9  ID  \n",
       "0  0.024609  0.011846   0  \n",
       "1  0.008814  0.004649   1  \n",
       "2  0.012332  0.008079   2  \n",
       "3  0.030713  0.014911   3  \n",
       "4  0.025380  0.008788   4  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df = pd.DataFrame(probas.numpy(), columns=['class'+str(c+1) for c in range(9)])\n",
    "submission_df['ID'] = df_test['ID']\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "py3.5",
   "language": "python",
   "name": "python35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
